{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqNS+8A2enYCwMpmrTE+ni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skhazaei/TensorFlow-repo/blob/master/text_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gu6zCYk9Y6H"
      },
      "source": [
        "##Generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucb7_GU7oPaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79334212-526c-4068-b67f-67004771c0e0"
      },
      "source": [
        "## the data is an irish song\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "print(data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In the town of Athy one Jeremy Lanigan \n",
            " Battered away til he hadnt a pound. \n",
            "His father died and made him a man again \n",
            " Left him a farm and ten acres of ground. \n",
            "He gave a grand party for friends and relations \n",
            "Who didnt forget him when come to the wall, \n",
            "And if youll but listen Ill make your eyes glisten \n",
            "Of the rows and the ructions of Lanigans Ball. \n",
            "Myself to be sure got free invitation, \n",
            "For all the nice girls and boys I might ask, \n",
            "And just in a minute both friends and relations \n",
            "Were dancing round merry as bees round a cask. \n",
            "Judy ODaly, that nice little milliner, \n",
            "She tipped me a wink for to give her a call, \n",
            "And I soon arrived with Peggy McGilligan \n",
            "Just in time for Lanigans Ball. \n",
            "There were lashings of punch and wine for the ladies, \n",
            "Potatoes and cakes; there was bacon and tea, \n",
            "There were the Nolans, Dolans, OGradys \n",
            "Courting the girls and dancing away. \n",
            "Songs they went round as plenty as water, \n",
            "The harp that once sounded in Taras old hall,\n",
            "Sweet Nelly Gray and The Rat Catchers Daughter,\n",
            "All singing together at Lanigans Ball. \n",
            "They were doing all kinds of nonsensical polkas \n",
            "All round the room in a whirligig. \n",
            "Julia and I, we banished their nonsense \n",
            "And tipped them the twist of a reel and a jig. \n",
            "Ach mavrone, how the girls got all mad at me \n",
            "Danced til youd think the ceiling would fall. \n",
            "For I spent three weeks at Brooks Academy \n",
            "Learning new steps for Lanigans Ball. \n",
            "Three long weeks I spent up in Dublin, \n",
            "Three long weeks to learn nothing at all,\n",
            " Three long weeks I spent up in Dublin, \n",
            "Learning new steps for Lanigans Ball. \n",
            "She stepped out and I stepped in again, \n",
            "I stepped out and she stepped in again, \n",
            "She stepped out and I stepped in again, \n",
            "Learning new steps for Lanigans Ball. \n",
            "Boys were all merry and the girls they were hearty \n",
            "And danced all around in couples and groups, \n",
            "Til an accident happened, young Terrance McCarthy \n",
            "Put his right leg through miss Finnertys hoops. \n",
            "Poor creature fainted and cried Meelia murther, \n",
            "Called for her brothers and gathered them all. \n",
            "Carmody swore that hed go no further \n",
            "Til he had satisfaction at Lanigans Ball. \n",
            "In the midst of the row miss Kerrigan fainted, \n",
            "Her cheeks at the same time as red as a rose. \n",
            "Some of the lads declared she was painted, \n",
            "She took a small drop too much, I suppose. \n",
            "Her sweetheart, Ned Morgan, so powerful and able, \n",
            "When he saw his fair colleen stretched out by the wall, \n",
            "Tore the left leg from under the table \n",
            "And smashed all the Chaneys at Lanigans Ball. \n",
            "Boys, oh boys, twas then there were runctions. \n",
            "Myself got a lick from big Phelim McHugh. \n",
            "I soon replied to his introduction \n",
            "And kicked up a terrible hullabaloo. \n",
            "Old Casey, the piper, was near being strangled. \n",
            "They squeezed up his pipes, bellows, chanters and all. \n",
            "The girls, in their ribbons, they got all entangled \n",
            "And that put an end to Lanigans Ball.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk4BMr8892Pf"
      },
      "source": [
        "## Let's generate a python list of sentences from the `data` and convert all to the lowercases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLiRM-TPozem",
        "outputId": "ba39e363-2899-4975-86e8-0c347df64ec5"
      },
      "source": [
        "corpus = data.lower().split('\\n')\n",
        "print('first sentence: {}'.format(corpus[0]))\n",
        "print('second sentence: {}'.format(corpus[1]))\n",
        "print('corpus size: {}'.format(len(corpus)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first sentence: in the town of athy one jeremy lanigan \n",
            "second sentence:  battered away til he hadnt a pound. \n",
            "corpus size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSR92y-n-8r5"
      },
      "source": [
        "## Using `Tokenizer`, create a dictionary of words from the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2RYE4fwo7Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1feafdc9-1acb-40fb-8a80-5aaf66a9c6d7"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "print('A dictionary of the words in the corpus:\\n {}'.format(word_index))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A dictionary of the words in the corpus:\n",
            " {'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VQeBOMvpGST",
        "outputId": "869e87af-0ad0-41ca-f707-d98789a03219"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index)+1\n",
        "print('total number of words in the dicionary, including out of vocabulary word: {}'.format(total_words))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of words in the dicionary, including out of vocabulary word: 263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM_MboY1A3ax"
      },
      "source": [
        "## Generate training data from the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIrPeK-ppb-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658b4e23-4d3d-4466-c511-7f116c13f97e"
      },
      "source": [
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  print(token_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 2, 66, 8, 67, 68, 69, 70]\n",
            "[71, 40, 20, 21, 72, 3, 73]\n",
            "[16, 74, 75, 1, 76, 33, 3, 77, 22]\n",
            "[41, 33, 3, 78, 1, 79, 80, 8, 81]\n",
            "[21, 82, 3, 83, 84, 7, 42, 1, 43]\n",
            "[85, 86, 87, 33, 44, 88, 13, 2, 45]\n",
            "[1, 89, 90, 91, 92, 93, 94, 95, 96, 97]\n",
            "[8, 2, 98, 1, 2, 99, 8, 9, 10]\n",
            "[46, 13, 100, 101, 23, 102, 103]\n",
            "[7, 5, 2, 47, 17, 1, 24, 6, 104, 105]\n",
            "[1, 48, 4, 3, 106, 107, 42, 1, 43]\n",
            "[11, 49, 25, 50, 18, 108, 25, 3, 109]\n",
            "[110, 111, 26, 47, 112, 113]\n",
            "[14, 51, 52, 3, 114, 7, 13, 115, 27, 3, 116]\n",
            "[1, 6, 53, 117, 118, 119, 120]\n",
            "[48, 4, 54, 7, 9, 10]\n",
            "[28, 11, 121, 8, 122, 1, 123, 7, 2, 124]\n",
            "[125, 1, 126, 28, 34, 127, 1, 128]\n",
            "[28, 11, 2, 129, 130, 131]\n",
            "[132, 2, 17, 1, 49, 40]\n",
            "[133, 19, 134, 25, 18, 135, 18, 136]\n",
            "[2, 137, 26, 138, 139, 4, 140, 55, 141]\n",
            "[142, 143, 144, 1, 2, 145, 146, 147]\n",
            "[5, 148, 149, 12, 9, 10]\n",
            "[19, 11, 150, 5, 151, 8, 152, 153]\n",
            "[5, 25, 2, 154, 4, 3, 155]\n",
            "[156, 1, 6, 157, 158, 56, 159]\n",
            "[1, 51, 57, 2, 160, 8, 3, 161, 1, 3, 162]\n",
            "[163, 164, 165, 2, 17, 23, 5, 166, 12, 52]\n",
            "[58, 20, 167, 168, 2, 169, 170, 171]\n",
            "[7, 6, 35, 29, 30, 12, 172, 173]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[29, 39, 30, 6, 35, 31, 4, 59]\n",
            "[29, 39, 30, 13, 174, 175, 12, 5]\n",
            "[29, 39, 30, 6, 35, 31, 4, 59]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[14, 15, 32, 1, 6, 15, 4, 22]\n",
            "[6, 15, 32, 1, 14, 15, 4, 22]\n",
            "[14, 15, 32, 1, 6, 15, 4, 22]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[24, 11, 5, 50, 1, 2, 17, 19, 11, 176]\n",
            "[1, 58, 5, 177, 4, 178, 1, 179]\n",
            "[20, 60, 180, 181, 182, 183, 184]\n",
            "[61, 16, 185, 62, 186, 63, 187, 188]\n",
            "[189, 190, 64, 1, 191, 192, 193]\n",
            "[194, 7, 27, 195, 1, 196, 57, 5]\n",
            "[197, 198, 26, 199, 200, 201, 202]\n",
            "[20, 21, 203, 204, 12, 9, 10]\n",
            "[4, 2, 205, 8, 2, 206, 63, 207, 64]\n",
            "[27, 208, 12, 2, 209, 54, 18, 210, 18, 3, 211]\n",
            "[212, 8, 2, 213, 214, 14, 34, 215]\n",
            "[14, 216, 3, 217, 218, 219, 220, 6, 221]\n",
            "[27, 222, 223, 224, 225, 226, 1, 227]\n",
            "[44, 21, 228, 16, 229, 230, 231, 32, 232, 2, 45]\n",
            "[233, 2, 41, 62, 65, 234, 2, 235]\n",
            "[1, 236, 5, 2, 237, 12, 9, 10]\n",
            "[24, 238, 24, 239, 240, 28, 11, 241]\n",
            "[46, 23, 3, 242, 65, 243, 244, 245]\n",
            "[6, 53, 246, 13, 16, 247]\n",
            "[1, 248, 31, 3, 249, 250]\n",
            "[55, 251, 2, 252, 34, 253, 254, 255]\n",
            "[19, 256, 31, 16, 257, 258, 259, 1, 5]\n",
            "[2, 17, 4, 56, 260, 19, 23, 5, 261]\n",
            "[1, 26, 61, 60, 262, 13, 9, 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcIevhzXBkT1",
        "outputId": "7040c626-7cc2-4759-a681-79bf2d394cab"
      },
      "source": [
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)+1):\n",
        "    sub_seq = token_list[:i+1]\n",
        "    print(sub_seq)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 2]\n",
            "[4, 2, 66]\n",
            "[4, 2, 66, 8]\n",
            "[4, 2, 66, 8, 67]\n",
            "[4, 2, 66, 8, 67, 68]\n",
            "[4, 2, 66, 8, 67, 68, 69]\n",
            "[4, 2, 66, 8, 67, 68, 69, 70]\n",
            "[4, 2, 66, 8, 67, 68, 69, 70]\n",
            "[71, 40]\n",
            "[71, 40, 20]\n",
            "[71, 40, 20, 21]\n",
            "[71, 40, 20, 21, 72]\n",
            "[71, 40, 20, 21, 72, 3]\n",
            "[71, 40, 20, 21, 72, 3, 73]\n",
            "[71, 40, 20, 21, 72, 3, 73]\n",
            "[16, 74]\n",
            "[16, 74, 75]\n",
            "[16, 74, 75, 1]\n",
            "[16, 74, 75, 1, 76]\n",
            "[16, 74, 75, 1, 76, 33]\n",
            "[16, 74, 75, 1, 76, 33, 3]\n",
            "[16, 74, 75, 1, 76, 33, 3, 77]\n",
            "[16, 74, 75, 1, 76, 33, 3, 77, 22]\n",
            "[16, 74, 75, 1, 76, 33, 3, 77, 22]\n",
            "[41, 33]\n",
            "[41, 33, 3]\n",
            "[41, 33, 3, 78]\n",
            "[41, 33, 3, 78, 1]\n",
            "[41, 33, 3, 78, 1, 79]\n",
            "[41, 33, 3, 78, 1, 79, 80]\n",
            "[41, 33, 3, 78, 1, 79, 80, 8]\n",
            "[41, 33, 3, 78, 1, 79, 80, 8, 81]\n",
            "[41, 33, 3, 78, 1, 79, 80, 8, 81]\n",
            "[21, 82]\n",
            "[21, 82, 3]\n",
            "[21, 82, 3, 83]\n",
            "[21, 82, 3, 83, 84]\n",
            "[21, 82, 3, 83, 84, 7]\n",
            "[21, 82, 3, 83, 84, 7, 42]\n",
            "[21, 82, 3, 83, 84, 7, 42, 1]\n",
            "[21, 82, 3, 83, 84, 7, 42, 1, 43]\n",
            "[21, 82, 3, 83, 84, 7, 42, 1, 43]\n",
            "[85, 86]\n",
            "[85, 86, 87]\n",
            "[85, 86, 87, 33]\n",
            "[85, 86, 87, 33, 44]\n",
            "[85, 86, 87, 33, 44, 88]\n",
            "[85, 86, 87, 33, 44, 88, 13]\n",
            "[85, 86, 87, 33, 44, 88, 13, 2]\n",
            "[85, 86, 87, 33, 44, 88, 13, 2, 45]\n",
            "[85, 86, 87, 33, 44, 88, 13, 2, 45]\n",
            "[1, 89]\n",
            "[1, 89, 90]\n",
            "[1, 89, 90, 91]\n",
            "[1, 89, 90, 91, 92]\n",
            "[1, 89, 90, 91, 92, 93]\n",
            "[1, 89, 90, 91, 92, 93, 94]\n",
            "[1, 89, 90, 91, 92, 93, 94, 95]\n",
            "[1, 89, 90, 91, 92, 93, 94, 95, 96]\n",
            "[1, 89, 90, 91, 92, 93, 94, 95, 96, 97]\n",
            "[1, 89, 90, 91, 92, 93, 94, 95, 96, 97]\n",
            "[8, 2]\n",
            "[8, 2, 98]\n",
            "[8, 2, 98, 1]\n",
            "[8, 2, 98, 1, 2]\n",
            "[8, 2, 98, 1, 2, 99]\n",
            "[8, 2, 98, 1, 2, 99, 8]\n",
            "[8, 2, 98, 1, 2, 99, 8, 9]\n",
            "[8, 2, 98, 1, 2, 99, 8, 9, 10]\n",
            "[8, 2, 98, 1, 2, 99, 8, 9, 10]\n",
            "[46, 13]\n",
            "[46, 13, 100]\n",
            "[46, 13, 100, 101]\n",
            "[46, 13, 100, 101, 23]\n",
            "[46, 13, 100, 101, 23, 102]\n",
            "[46, 13, 100, 101, 23, 102, 103]\n",
            "[46, 13, 100, 101, 23, 102, 103]\n",
            "[7, 5]\n",
            "[7, 5, 2]\n",
            "[7, 5, 2, 47]\n",
            "[7, 5, 2, 47, 17]\n",
            "[7, 5, 2, 47, 17, 1]\n",
            "[7, 5, 2, 47, 17, 1, 24]\n",
            "[7, 5, 2, 47, 17, 1, 24, 6]\n",
            "[7, 5, 2, 47, 17, 1, 24, 6, 104]\n",
            "[7, 5, 2, 47, 17, 1, 24, 6, 104, 105]\n",
            "[7, 5, 2, 47, 17, 1, 24, 6, 104, 105]\n",
            "[1, 48]\n",
            "[1, 48, 4]\n",
            "[1, 48, 4, 3]\n",
            "[1, 48, 4, 3, 106]\n",
            "[1, 48, 4, 3, 106, 107]\n",
            "[1, 48, 4, 3, 106, 107, 42]\n",
            "[1, 48, 4, 3, 106, 107, 42, 1]\n",
            "[1, 48, 4, 3, 106, 107, 42, 1, 43]\n",
            "[1, 48, 4, 3, 106, 107, 42, 1, 43]\n",
            "[11, 49]\n",
            "[11, 49, 25]\n",
            "[11, 49, 25, 50]\n",
            "[11, 49, 25, 50, 18]\n",
            "[11, 49, 25, 50, 18, 108]\n",
            "[11, 49, 25, 50, 18, 108, 25]\n",
            "[11, 49, 25, 50, 18, 108, 25, 3]\n",
            "[11, 49, 25, 50, 18, 108, 25, 3, 109]\n",
            "[11, 49, 25, 50, 18, 108, 25, 3, 109]\n",
            "[110, 111]\n",
            "[110, 111, 26]\n",
            "[110, 111, 26, 47]\n",
            "[110, 111, 26, 47, 112]\n",
            "[110, 111, 26, 47, 112, 113]\n",
            "[110, 111, 26, 47, 112, 113]\n",
            "[14, 51]\n",
            "[14, 51, 52]\n",
            "[14, 51, 52, 3]\n",
            "[14, 51, 52, 3, 114]\n",
            "[14, 51, 52, 3, 114, 7]\n",
            "[14, 51, 52, 3, 114, 7, 13]\n",
            "[14, 51, 52, 3, 114, 7, 13, 115]\n",
            "[14, 51, 52, 3, 114, 7, 13, 115, 27]\n",
            "[14, 51, 52, 3, 114, 7, 13, 115, 27, 3]\n",
            "[14, 51, 52, 3, 114, 7, 13, 115, 27, 3, 116]\n",
            "[14, 51, 52, 3, 114, 7, 13, 115, 27, 3, 116]\n",
            "[1, 6]\n",
            "[1, 6, 53]\n",
            "[1, 6, 53, 117]\n",
            "[1, 6, 53, 117, 118]\n",
            "[1, 6, 53, 117, 118, 119]\n",
            "[1, 6, 53, 117, 118, 119, 120]\n",
            "[1, 6, 53, 117, 118, 119, 120]\n",
            "[48, 4]\n",
            "[48, 4, 54]\n",
            "[48, 4, 54, 7]\n",
            "[48, 4, 54, 7, 9]\n",
            "[48, 4, 54, 7, 9, 10]\n",
            "[48, 4, 54, 7, 9, 10]\n",
            "[28, 11]\n",
            "[28, 11, 121]\n",
            "[28, 11, 121, 8]\n",
            "[28, 11, 121, 8, 122]\n",
            "[28, 11, 121, 8, 122, 1]\n",
            "[28, 11, 121, 8, 122, 1, 123]\n",
            "[28, 11, 121, 8, 122, 1, 123, 7]\n",
            "[28, 11, 121, 8, 122, 1, 123, 7, 2]\n",
            "[28, 11, 121, 8, 122, 1, 123, 7, 2, 124]\n",
            "[28, 11, 121, 8, 122, 1, 123, 7, 2, 124]\n",
            "[125, 1]\n",
            "[125, 1, 126]\n",
            "[125, 1, 126, 28]\n",
            "[125, 1, 126, 28, 34]\n",
            "[125, 1, 126, 28, 34, 127]\n",
            "[125, 1, 126, 28, 34, 127, 1]\n",
            "[125, 1, 126, 28, 34, 127, 1, 128]\n",
            "[125, 1, 126, 28, 34, 127, 1, 128]\n",
            "[28, 11]\n",
            "[28, 11, 2]\n",
            "[28, 11, 2, 129]\n",
            "[28, 11, 2, 129, 130]\n",
            "[28, 11, 2, 129, 130, 131]\n",
            "[28, 11, 2, 129, 130, 131]\n",
            "[132, 2]\n",
            "[132, 2, 17]\n",
            "[132, 2, 17, 1]\n",
            "[132, 2, 17, 1, 49]\n",
            "[132, 2, 17, 1, 49, 40]\n",
            "[132, 2, 17, 1, 49, 40]\n",
            "[133, 19]\n",
            "[133, 19, 134]\n",
            "[133, 19, 134, 25]\n",
            "[133, 19, 134, 25, 18]\n",
            "[133, 19, 134, 25, 18, 135]\n",
            "[133, 19, 134, 25, 18, 135, 18]\n",
            "[133, 19, 134, 25, 18, 135, 18, 136]\n",
            "[133, 19, 134, 25, 18, 135, 18, 136]\n",
            "[2, 137]\n",
            "[2, 137, 26]\n",
            "[2, 137, 26, 138]\n",
            "[2, 137, 26, 138, 139]\n",
            "[2, 137, 26, 138, 139, 4]\n",
            "[2, 137, 26, 138, 139, 4, 140]\n",
            "[2, 137, 26, 138, 139, 4, 140, 55]\n",
            "[2, 137, 26, 138, 139, 4, 140, 55, 141]\n",
            "[2, 137, 26, 138, 139, 4, 140, 55, 141]\n",
            "[142, 143]\n",
            "[142, 143, 144]\n",
            "[142, 143, 144, 1]\n",
            "[142, 143, 144, 1, 2]\n",
            "[142, 143, 144, 1, 2, 145]\n",
            "[142, 143, 144, 1, 2, 145, 146]\n",
            "[142, 143, 144, 1, 2, 145, 146, 147]\n",
            "[142, 143, 144, 1, 2, 145, 146, 147]\n",
            "[5, 148]\n",
            "[5, 148, 149]\n",
            "[5, 148, 149, 12]\n",
            "[5, 148, 149, 12, 9]\n",
            "[5, 148, 149, 12, 9, 10]\n",
            "[5, 148, 149, 12, 9, 10]\n",
            "[19, 11]\n",
            "[19, 11, 150]\n",
            "[19, 11, 150, 5]\n",
            "[19, 11, 150, 5, 151]\n",
            "[19, 11, 150, 5, 151, 8]\n",
            "[19, 11, 150, 5, 151, 8, 152]\n",
            "[19, 11, 150, 5, 151, 8, 152, 153]\n",
            "[19, 11, 150, 5, 151, 8, 152, 153]\n",
            "[5, 25]\n",
            "[5, 25, 2]\n",
            "[5, 25, 2, 154]\n",
            "[5, 25, 2, 154, 4]\n",
            "[5, 25, 2, 154, 4, 3]\n",
            "[5, 25, 2, 154, 4, 3, 155]\n",
            "[5, 25, 2, 154, 4, 3, 155]\n",
            "[156, 1]\n",
            "[156, 1, 6]\n",
            "[156, 1, 6, 157]\n",
            "[156, 1, 6, 157, 158]\n",
            "[156, 1, 6, 157, 158, 56]\n",
            "[156, 1, 6, 157, 158, 56, 159]\n",
            "[156, 1, 6, 157, 158, 56, 159]\n",
            "[1, 51]\n",
            "[1, 51, 57]\n",
            "[1, 51, 57, 2]\n",
            "[1, 51, 57, 2, 160]\n",
            "[1, 51, 57, 2, 160, 8]\n",
            "[1, 51, 57, 2, 160, 8, 3]\n",
            "[1, 51, 57, 2, 160, 8, 3, 161]\n",
            "[1, 51, 57, 2, 160, 8, 3, 161, 1]\n",
            "[1, 51, 57, 2, 160, 8, 3, 161, 1, 3]\n",
            "[1, 51, 57, 2, 160, 8, 3, 161, 1, 3, 162]\n",
            "[1, 51, 57, 2, 160, 8, 3, 161, 1, 3, 162]\n",
            "[163, 164]\n",
            "[163, 164, 165]\n",
            "[163, 164, 165, 2]\n",
            "[163, 164, 165, 2, 17]\n",
            "[163, 164, 165, 2, 17, 23]\n",
            "[163, 164, 165, 2, 17, 23, 5]\n",
            "[163, 164, 165, 2, 17, 23, 5, 166]\n",
            "[163, 164, 165, 2, 17, 23, 5, 166, 12]\n",
            "[163, 164, 165, 2, 17, 23, 5, 166, 12, 52]\n",
            "[163, 164, 165, 2, 17, 23, 5, 166, 12, 52]\n",
            "[58, 20]\n",
            "[58, 20, 167]\n",
            "[58, 20, 167, 168]\n",
            "[58, 20, 167, 168, 2]\n",
            "[58, 20, 167, 168, 2, 169]\n",
            "[58, 20, 167, 168, 2, 169, 170]\n",
            "[58, 20, 167, 168, 2, 169, 170, 171]\n",
            "[58, 20, 167, 168, 2, 169, 170, 171]\n",
            "[7, 6]\n",
            "[7, 6, 35]\n",
            "[7, 6, 35, 29]\n",
            "[7, 6, 35, 29, 30]\n",
            "[7, 6, 35, 29, 30, 12]\n",
            "[7, 6, 35, 29, 30, 12, 172]\n",
            "[7, 6, 35, 29, 30, 12, 172, 173]\n",
            "[7, 6, 35, 29, 30, 12, 172, 173]\n",
            "[36, 37]\n",
            "[36, 37, 38]\n",
            "[36, 37, 38, 7]\n",
            "[36, 37, 38, 7, 9]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[29, 39]\n",
            "[29, 39, 30]\n",
            "[29, 39, 30, 6]\n",
            "[29, 39, 30, 6, 35]\n",
            "[29, 39, 30, 6, 35, 31]\n",
            "[29, 39, 30, 6, 35, 31, 4]\n",
            "[29, 39, 30, 6, 35, 31, 4, 59]\n",
            "[29, 39, 30, 6, 35, 31, 4, 59]\n",
            "[29, 39]\n",
            "[29, 39, 30]\n",
            "[29, 39, 30, 13]\n",
            "[29, 39, 30, 13, 174]\n",
            "[29, 39, 30, 13, 174, 175]\n",
            "[29, 39, 30, 13, 174, 175, 12]\n",
            "[29, 39, 30, 13, 174, 175, 12, 5]\n",
            "[29, 39, 30, 13, 174, 175, 12, 5]\n",
            "[29, 39]\n",
            "[29, 39, 30]\n",
            "[29, 39, 30, 6]\n",
            "[29, 39, 30, 6, 35]\n",
            "[29, 39, 30, 6, 35, 31]\n",
            "[29, 39, 30, 6, 35, 31, 4]\n",
            "[29, 39, 30, 6, 35, 31, 4, 59]\n",
            "[29, 39, 30, 6, 35, 31, 4, 59]\n",
            "[36, 37]\n",
            "[36, 37, 38]\n",
            "[36, 37, 38, 7]\n",
            "[36, 37, 38, 7, 9]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[14, 15]\n",
            "[14, 15, 32]\n",
            "[14, 15, 32, 1]\n",
            "[14, 15, 32, 1, 6]\n",
            "[14, 15, 32, 1, 6, 15]\n",
            "[14, 15, 32, 1, 6, 15, 4]\n",
            "[14, 15, 32, 1, 6, 15, 4, 22]\n",
            "[14, 15, 32, 1, 6, 15, 4, 22]\n",
            "[6, 15]\n",
            "[6, 15, 32]\n",
            "[6, 15, 32, 1]\n",
            "[6, 15, 32, 1, 14]\n",
            "[6, 15, 32, 1, 14, 15]\n",
            "[6, 15, 32, 1, 14, 15, 4]\n",
            "[6, 15, 32, 1, 14, 15, 4, 22]\n",
            "[6, 15, 32, 1, 14, 15, 4, 22]\n",
            "[14, 15]\n",
            "[14, 15, 32]\n",
            "[14, 15, 32, 1]\n",
            "[14, 15, 32, 1, 6]\n",
            "[14, 15, 32, 1, 6, 15]\n",
            "[14, 15, 32, 1, 6, 15, 4]\n",
            "[14, 15, 32, 1, 6, 15, 4, 22]\n",
            "[14, 15, 32, 1, 6, 15, 4, 22]\n",
            "[36, 37]\n",
            "[36, 37, 38]\n",
            "[36, 37, 38, 7]\n",
            "[36, 37, 38, 7, 9]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[36, 37, 38, 7, 9, 10]\n",
            "[24, 11]\n",
            "[24, 11, 5]\n",
            "[24, 11, 5, 50]\n",
            "[24, 11, 5, 50, 1]\n",
            "[24, 11, 5, 50, 1, 2]\n",
            "[24, 11, 5, 50, 1, 2, 17]\n",
            "[24, 11, 5, 50, 1, 2, 17, 19]\n",
            "[24, 11, 5, 50, 1, 2, 17, 19, 11]\n",
            "[24, 11, 5, 50, 1, 2, 17, 19, 11, 176]\n",
            "[24, 11, 5, 50, 1, 2, 17, 19, 11, 176]\n",
            "[1, 58]\n",
            "[1, 58, 5]\n",
            "[1, 58, 5, 177]\n",
            "[1, 58, 5, 177, 4]\n",
            "[1, 58, 5, 177, 4, 178]\n",
            "[1, 58, 5, 177, 4, 178, 1]\n",
            "[1, 58, 5, 177, 4, 178, 1, 179]\n",
            "[1, 58, 5, 177, 4, 178, 1, 179]\n",
            "[20, 60]\n",
            "[20, 60, 180]\n",
            "[20, 60, 180, 181]\n",
            "[20, 60, 180, 181, 182]\n",
            "[20, 60, 180, 181, 182, 183]\n",
            "[20, 60, 180, 181, 182, 183, 184]\n",
            "[20, 60, 180, 181, 182, 183, 184]\n",
            "[61, 16]\n",
            "[61, 16, 185]\n",
            "[61, 16, 185, 62]\n",
            "[61, 16, 185, 62, 186]\n",
            "[61, 16, 185, 62, 186, 63]\n",
            "[61, 16, 185, 62, 186, 63, 187]\n",
            "[61, 16, 185, 62, 186, 63, 187, 188]\n",
            "[61, 16, 185, 62, 186, 63, 187, 188]\n",
            "[189, 190]\n",
            "[189, 190, 64]\n",
            "[189, 190, 64, 1]\n",
            "[189, 190, 64, 1, 191]\n",
            "[189, 190, 64, 1, 191, 192]\n",
            "[189, 190, 64, 1, 191, 192, 193]\n",
            "[189, 190, 64, 1, 191, 192, 193]\n",
            "[194, 7]\n",
            "[194, 7, 27]\n",
            "[194, 7, 27, 195]\n",
            "[194, 7, 27, 195, 1]\n",
            "[194, 7, 27, 195, 1, 196]\n",
            "[194, 7, 27, 195, 1, 196, 57]\n",
            "[194, 7, 27, 195, 1, 196, 57, 5]\n",
            "[194, 7, 27, 195, 1, 196, 57, 5]\n",
            "[197, 198]\n",
            "[197, 198, 26]\n",
            "[197, 198, 26, 199]\n",
            "[197, 198, 26, 199, 200]\n",
            "[197, 198, 26, 199, 200, 201]\n",
            "[197, 198, 26, 199, 200, 201, 202]\n",
            "[197, 198, 26, 199, 200, 201, 202]\n",
            "[20, 21]\n",
            "[20, 21, 203]\n",
            "[20, 21, 203, 204]\n",
            "[20, 21, 203, 204, 12]\n",
            "[20, 21, 203, 204, 12, 9]\n",
            "[20, 21, 203, 204, 12, 9, 10]\n",
            "[20, 21, 203, 204, 12, 9, 10]\n",
            "[4, 2]\n",
            "[4, 2, 205]\n",
            "[4, 2, 205, 8]\n",
            "[4, 2, 205, 8, 2]\n",
            "[4, 2, 205, 8, 2, 206]\n",
            "[4, 2, 205, 8, 2, 206, 63]\n",
            "[4, 2, 205, 8, 2, 206, 63, 207]\n",
            "[4, 2, 205, 8, 2, 206, 63, 207, 64]\n",
            "[4, 2, 205, 8, 2, 206, 63, 207, 64]\n",
            "[27, 208]\n",
            "[27, 208, 12]\n",
            "[27, 208, 12, 2]\n",
            "[27, 208, 12, 2, 209]\n",
            "[27, 208, 12, 2, 209, 54]\n",
            "[27, 208, 12, 2, 209, 54, 18]\n",
            "[27, 208, 12, 2, 209, 54, 18, 210]\n",
            "[27, 208, 12, 2, 209, 54, 18, 210, 18]\n",
            "[27, 208, 12, 2, 209, 54, 18, 210, 18, 3]\n",
            "[27, 208, 12, 2, 209, 54, 18, 210, 18, 3, 211]\n",
            "[27, 208, 12, 2, 209, 54, 18, 210, 18, 3, 211]\n",
            "[212, 8]\n",
            "[212, 8, 2]\n",
            "[212, 8, 2, 213]\n",
            "[212, 8, 2, 213, 214]\n",
            "[212, 8, 2, 213, 214, 14]\n",
            "[212, 8, 2, 213, 214, 14, 34]\n",
            "[212, 8, 2, 213, 214, 14, 34, 215]\n",
            "[212, 8, 2, 213, 214, 14, 34, 215]\n",
            "[14, 216]\n",
            "[14, 216, 3]\n",
            "[14, 216, 3, 217]\n",
            "[14, 216, 3, 217, 218]\n",
            "[14, 216, 3, 217, 218, 219]\n",
            "[14, 216, 3, 217, 218, 219, 220]\n",
            "[14, 216, 3, 217, 218, 219, 220, 6]\n",
            "[14, 216, 3, 217, 218, 219, 220, 6, 221]\n",
            "[14, 216, 3, 217, 218, 219, 220, 6, 221]\n",
            "[27, 222]\n",
            "[27, 222, 223]\n",
            "[27, 222, 223, 224]\n",
            "[27, 222, 223, 224, 225]\n",
            "[27, 222, 223, 224, 225, 226]\n",
            "[27, 222, 223, 224, 225, 226, 1]\n",
            "[27, 222, 223, 224, 225, 226, 1, 227]\n",
            "[27, 222, 223, 224, 225, 226, 1, 227]\n",
            "[44, 21]\n",
            "[44, 21, 228]\n",
            "[44, 21, 228, 16]\n",
            "[44, 21, 228, 16, 229]\n",
            "[44, 21, 228, 16, 229, 230]\n",
            "[44, 21, 228, 16, 229, 230, 231]\n",
            "[44, 21, 228, 16, 229, 230, 231, 32]\n",
            "[44, 21, 228, 16, 229, 230, 231, 32, 232]\n",
            "[44, 21, 228, 16, 229, 230, 231, 32, 232, 2]\n",
            "[44, 21, 228, 16, 229, 230, 231, 32, 232, 2, 45]\n",
            "[44, 21, 228, 16, 229, 230, 231, 32, 232, 2, 45]\n",
            "[233, 2]\n",
            "[233, 2, 41]\n",
            "[233, 2, 41, 62]\n",
            "[233, 2, 41, 62, 65]\n",
            "[233, 2, 41, 62, 65, 234]\n",
            "[233, 2, 41, 62, 65, 234, 2]\n",
            "[233, 2, 41, 62, 65, 234, 2, 235]\n",
            "[233, 2, 41, 62, 65, 234, 2, 235]\n",
            "[1, 236]\n",
            "[1, 236, 5]\n",
            "[1, 236, 5, 2]\n",
            "[1, 236, 5, 2, 237]\n",
            "[1, 236, 5, 2, 237, 12]\n",
            "[1, 236, 5, 2, 237, 12, 9]\n",
            "[1, 236, 5, 2, 237, 12, 9, 10]\n",
            "[1, 236, 5, 2, 237, 12, 9, 10]\n",
            "[24, 238]\n",
            "[24, 238, 24]\n",
            "[24, 238, 24, 239]\n",
            "[24, 238, 24, 239, 240]\n",
            "[24, 238, 24, 239, 240, 28]\n",
            "[24, 238, 24, 239, 240, 28, 11]\n",
            "[24, 238, 24, 239, 240, 28, 11, 241]\n",
            "[24, 238, 24, 239, 240, 28, 11, 241]\n",
            "[46, 23]\n",
            "[46, 23, 3]\n",
            "[46, 23, 3, 242]\n",
            "[46, 23, 3, 242, 65]\n",
            "[46, 23, 3, 242, 65, 243]\n",
            "[46, 23, 3, 242, 65, 243, 244]\n",
            "[46, 23, 3, 242, 65, 243, 244, 245]\n",
            "[46, 23, 3, 242, 65, 243, 244, 245]\n",
            "[6, 53]\n",
            "[6, 53, 246]\n",
            "[6, 53, 246, 13]\n",
            "[6, 53, 246, 13, 16]\n",
            "[6, 53, 246, 13, 16, 247]\n",
            "[6, 53, 246, 13, 16, 247]\n",
            "[1, 248]\n",
            "[1, 248, 31]\n",
            "[1, 248, 31, 3]\n",
            "[1, 248, 31, 3, 249]\n",
            "[1, 248, 31, 3, 249, 250]\n",
            "[1, 248, 31, 3, 249, 250]\n",
            "[55, 251]\n",
            "[55, 251, 2]\n",
            "[55, 251, 2, 252]\n",
            "[55, 251, 2, 252, 34]\n",
            "[55, 251, 2, 252, 34, 253]\n",
            "[55, 251, 2, 252, 34, 253, 254]\n",
            "[55, 251, 2, 252, 34, 253, 254, 255]\n",
            "[55, 251, 2, 252, 34, 253, 254, 255]\n",
            "[19, 256]\n",
            "[19, 256, 31]\n",
            "[19, 256, 31, 16]\n",
            "[19, 256, 31, 16, 257]\n",
            "[19, 256, 31, 16, 257, 258]\n",
            "[19, 256, 31, 16, 257, 258, 259]\n",
            "[19, 256, 31, 16, 257, 258, 259, 1]\n",
            "[19, 256, 31, 16, 257, 258, 259, 1, 5]\n",
            "[19, 256, 31, 16, 257, 258, 259, 1, 5]\n",
            "[2, 17]\n",
            "[2, 17, 4]\n",
            "[2, 17, 4, 56]\n",
            "[2, 17, 4, 56, 260]\n",
            "[2, 17, 4, 56, 260, 19]\n",
            "[2, 17, 4, 56, 260, 19, 23]\n",
            "[2, 17, 4, 56, 260, 19, 23, 5]\n",
            "[2, 17, 4, 56, 260, 19, 23, 5, 261]\n",
            "[2, 17, 4, 56, 260, 19, 23, 5, 261]\n",
            "[1, 26]\n",
            "[1, 26, 61]\n",
            "[1, 26, 61, 60]\n",
            "[1, 26, 61, 60, 262]\n",
            "[1, 26, 61, 60, 262, 13]\n",
            "[1, 26, 61, 60, 262, 13, 9]\n",
            "[1, 26, 61, 60, 262, 13, 9, 10]\n",
            "[1, 26, 61, 60, 262, 13, 9, 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcDolBJrD5Lm",
        "outputId": "bf04ca77-c15b-4fdd-ef47-e1a964215c4e"
      },
      "source": [
        "input_sequences=[]\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)+1):\n",
        "    sub_seq = token_list[:i+1]\n",
        "    input_sequences.append(sub_seq)\n",
        "print(input_sequences)\n",
        "print('size of training set: {}'.format(len(input_sequences)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2], [4, 2, 66], [4, 2, 66, 8], [4, 2, 66, 8, 67], [4, 2, 66, 8, 67, 68], [4, 2, 66, 8, 67, 68, 69], [4, 2, 66, 8, 67, 68, 69, 70], [4, 2, 66, 8, 67, 68, 69, 70], [71, 40], [71, 40, 20], [71, 40, 20, 21], [71, 40, 20, 21, 72], [71, 40, 20, 21, 72, 3], [71, 40, 20, 21, 72, 3, 73], [71, 40, 20, 21, 72, 3, 73], [16, 74], [16, 74, 75], [16, 74, 75, 1], [16, 74, 75, 1, 76], [16, 74, 75, 1, 76, 33], [16, 74, 75, 1, 76, 33, 3], [16, 74, 75, 1, 76, 33, 3, 77], [16, 74, 75, 1, 76, 33, 3, 77, 22], [16, 74, 75, 1, 76, 33, 3, 77, 22], [41, 33], [41, 33, 3], [41, 33, 3, 78], [41, 33, 3, 78, 1], [41, 33, 3, 78, 1, 79], [41, 33, 3, 78, 1, 79, 80], [41, 33, 3, 78, 1, 79, 80, 8], [41, 33, 3, 78, 1, 79, 80, 8, 81], [41, 33, 3, 78, 1, 79, 80, 8, 81], [21, 82], [21, 82, 3], [21, 82, 3, 83], [21, 82, 3, 83, 84], [21, 82, 3, 83, 84, 7], [21, 82, 3, 83, 84, 7, 42], [21, 82, 3, 83, 84, 7, 42, 1], [21, 82, 3, 83, 84, 7, 42, 1, 43], [21, 82, 3, 83, 84, 7, 42, 1, 43], [85, 86], [85, 86, 87], [85, 86, 87, 33], [85, 86, 87, 33, 44], [85, 86, 87, 33, 44, 88], [85, 86, 87, 33, 44, 88, 13], [85, 86, 87, 33, 44, 88, 13, 2], [85, 86, 87, 33, 44, 88, 13, 2, 45], [85, 86, 87, 33, 44, 88, 13, 2, 45], [1, 89], [1, 89, 90], [1, 89, 90, 91], [1, 89, 90, 91, 92], [1, 89, 90, 91, 92, 93], [1, 89, 90, 91, 92, 93, 94], [1, 89, 90, 91, 92, 93, 94, 95], [1, 89, 90, 91, 92, 93, 94, 95, 96], [1, 89, 90, 91, 92, 93, 94, 95, 96, 97], [1, 89, 90, 91, 92, 93, 94, 95, 96, 97], [8, 2], [8, 2, 98], [8, 2, 98, 1], [8, 2, 98, 1, 2], [8, 2, 98, 1, 2, 99], [8, 2, 98, 1, 2, 99, 8], [8, 2, 98, 1, 2, 99, 8, 9], [8, 2, 98, 1, 2, 99, 8, 9, 10], [8, 2, 98, 1, 2, 99, 8, 9, 10], [46, 13], [46, 13, 100], [46, 13, 100, 101], [46, 13, 100, 101, 23], [46, 13, 100, 101, 23, 102], [46, 13, 100, 101, 23, 102, 103], [46, 13, 100, 101, 23, 102, 103], [7, 5], [7, 5, 2], [7, 5, 2, 47], [7, 5, 2, 47, 17], [7, 5, 2, 47, 17, 1], [7, 5, 2, 47, 17, 1, 24], [7, 5, 2, 47, 17, 1, 24, 6], [7, 5, 2, 47, 17, 1, 24, 6, 104], [7, 5, 2, 47, 17, 1, 24, 6, 104, 105], [7, 5, 2, 47, 17, 1, 24, 6, 104, 105], [1, 48], [1, 48, 4], [1, 48, 4, 3], [1, 48, 4, 3, 106], [1, 48, 4, 3, 106, 107], [1, 48, 4, 3, 106, 107, 42], [1, 48, 4, 3, 106, 107, 42, 1], [1, 48, 4, 3, 106, 107, 42, 1, 43], [1, 48, 4, 3, 106, 107, 42, 1, 43], [11, 49], [11, 49, 25], [11, 49, 25, 50], [11, 49, 25, 50, 18], [11, 49, 25, 50, 18, 108], [11, 49, 25, 50, 18, 108, 25], [11, 49, 25, 50, 18, 108, 25, 3], [11, 49, 25, 50, 18, 108, 25, 3, 109], [11, 49, 25, 50, 18, 108, 25, 3, 109], [110, 111], [110, 111, 26], [110, 111, 26, 47], [110, 111, 26, 47, 112], [110, 111, 26, 47, 112, 113], [110, 111, 26, 47, 112, 113], [14, 51], [14, 51, 52], [14, 51, 52, 3], [14, 51, 52, 3, 114], [14, 51, 52, 3, 114, 7], [14, 51, 52, 3, 114, 7, 13], [14, 51, 52, 3, 114, 7, 13, 115], [14, 51, 52, 3, 114, 7, 13, 115, 27], [14, 51, 52, 3, 114, 7, 13, 115, 27, 3], [14, 51, 52, 3, 114, 7, 13, 115, 27, 3, 116], [14, 51, 52, 3, 114, 7, 13, 115, 27, 3, 116], [1, 6], [1, 6, 53], [1, 6, 53, 117], [1, 6, 53, 117, 118], [1, 6, 53, 117, 118, 119], [1, 6, 53, 117, 118, 119, 120], [1, 6, 53, 117, 118, 119, 120], [48, 4], [48, 4, 54], [48, 4, 54, 7], [48, 4, 54, 7, 9], [48, 4, 54, 7, 9, 10], [48, 4, 54, 7, 9, 10], [28, 11], [28, 11, 121], [28, 11, 121, 8], [28, 11, 121, 8, 122], [28, 11, 121, 8, 122, 1], [28, 11, 121, 8, 122, 1, 123], [28, 11, 121, 8, 122, 1, 123, 7], [28, 11, 121, 8, 122, 1, 123, 7, 2], [28, 11, 121, 8, 122, 1, 123, 7, 2, 124], [28, 11, 121, 8, 122, 1, 123, 7, 2, 124], [125, 1], [125, 1, 126], [125, 1, 126, 28], [125, 1, 126, 28, 34], [125, 1, 126, 28, 34, 127], [125, 1, 126, 28, 34, 127, 1], [125, 1, 126, 28, 34, 127, 1, 128], [125, 1, 126, 28, 34, 127, 1, 128], [28, 11], [28, 11, 2], [28, 11, 2, 129], [28, 11, 2, 129, 130], [28, 11, 2, 129, 130, 131], [28, 11, 2, 129, 130, 131], [132, 2], [132, 2, 17], [132, 2, 17, 1], [132, 2, 17, 1, 49], [132, 2, 17, 1, 49, 40], [132, 2, 17, 1, 49, 40], [133, 19], [133, 19, 134], [133, 19, 134, 25], [133, 19, 134, 25, 18], [133, 19, 134, 25, 18, 135], [133, 19, 134, 25, 18, 135, 18], [133, 19, 134, 25, 18, 135, 18, 136], [133, 19, 134, 25, 18, 135, 18, 136], [2, 137], [2, 137, 26], [2, 137, 26, 138], [2, 137, 26, 138, 139], [2, 137, 26, 138, 139, 4], [2, 137, 26, 138, 139, 4, 140], [2, 137, 26, 138, 139, 4, 140, 55], [2, 137, 26, 138, 139, 4, 140, 55, 141], [2, 137, 26, 138, 139, 4, 140, 55, 141], [142, 143], [142, 143, 144], [142, 143, 144, 1], [142, 143, 144, 1, 2], [142, 143, 144, 1, 2, 145], [142, 143, 144, 1, 2, 145, 146], [142, 143, 144, 1, 2, 145, 146, 147], [142, 143, 144, 1, 2, 145, 146, 147], [5, 148], [5, 148, 149], [5, 148, 149, 12], [5, 148, 149, 12, 9], [5, 148, 149, 12, 9, 10], [5, 148, 149, 12, 9, 10], [19, 11], [19, 11, 150], [19, 11, 150, 5], [19, 11, 150, 5, 151], [19, 11, 150, 5, 151, 8], [19, 11, 150, 5, 151, 8, 152], [19, 11, 150, 5, 151, 8, 152, 153], [19, 11, 150, 5, 151, 8, 152, 153], [5, 25], [5, 25, 2], [5, 25, 2, 154], [5, 25, 2, 154, 4], [5, 25, 2, 154, 4, 3], [5, 25, 2, 154, 4, 3, 155], [5, 25, 2, 154, 4, 3, 155], [156, 1], [156, 1, 6], [156, 1, 6, 157], [156, 1, 6, 157, 158], [156, 1, 6, 157, 158, 56], [156, 1, 6, 157, 158, 56, 159], [156, 1, 6, 157, 158, 56, 159], [1, 51], [1, 51, 57], [1, 51, 57, 2], [1, 51, 57, 2, 160], [1, 51, 57, 2, 160, 8], [1, 51, 57, 2, 160, 8, 3], [1, 51, 57, 2, 160, 8, 3, 161], [1, 51, 57, 2, 160, 8, 3, 161, 1], [1, 51, 57, 2, 160, 8, 3, 161, 1, 3], [1, 51, 57, 2, 160, 8, 3, 161, 1, 3, 162], [1, 51, 57, 2, 160, 8, 3, 161, 1, 3, 162], [163, 164], [163, 164, 165], [163, 164, 165, 2], [163, 164, 165, 2, 17], [163, 164, 165, 2, 17, 23], [163, 164, 165, 2, 17, 23, 5], [163, 164, 165, 2, 17, 23, 5, 166], [163, 164, 165, 2, 17, 23, 5, 166, 12], [163, 164, 165, 2, 17, 23, 5, 166, 12, 52], [163, 164, 165, 2, 17, 23, 5, 166, 12, 52], [58, 20], [58, 20, 167], [58, 20, 167, 168], [58, 20, 167, 168, 2], [58, 20, 167, 168, 2, 169], [58, 20, 167, 168, 2, 169, 170], [58, 20, 167, 168, 2, 169, 170, 171], [58, 20, 167, 168, 2, 169, 170, 171], [7, 6], [7, 6, 35], [7, 6, 35, 29], [7, 6, 35, 29, 30], [7, 6, 35, 29, 30, 12], [7, 6, 35, 29, 30, 12, 172], [7, 6, 35, 29, 30, 12, 172, 173], [7, 6, 35, 29, 30, 12, 172, 173], [36, 37], [36, 37, 38], [36, 37, 38, 7], [36, 37, 38, 7, 9], [36, 37, 38, 7, 9, 10], [36, 37, 38, 7, 9, 10], [29, 39], [29, 39, 30], [29, 39, 30, 6], [29, 39, 30, 6, 35], [29, 39, 30, 6, 35, 31], [29, 39, 30, 6, 35, 31, 4], [29, 39, 30, 6, 35, 31, 4, 59], [29, 39, 30, 6, 35, 31, 4, 59], [29, 39], [29, 39, 30], [29, 39, 30, 13], [29, 39, 30, 13, 174], [29, 39, 30, 13, 174, 175], [29, 39, 30, 13, 174, 175, 12], [29, 39, 30, 13, 174, 175, 12, 5], [29, 39, 30, 13, 174, 175, 12, 5], [29, 39], [29, 39, 30], [29, 39, 30, 6], [29, 39, 30, 6, 35], [29, 39, 30, 6, 35, 31], [29, 39, 30, 6, 35, 31, 4], [29, 39, 30, 6, 35, 31, 4, 59], [29, 39, 30, 6, 35, 31, 4, 59], [36, 37], [36, 37, 38], [36, 37, 38, 7], [36, 37, 38, 7, 9], [36, 37, 38, 7, 9, 10], [36, 37, 38, 7, 9, 10], [14, 15], [14, 15, 32], [14, 15, 32, 1], [14, 15, 32, 1, 6], [14, 15, 32, 1, 6, 15], [14, 15, 32, 1, 6, 15, 4], [14, 15, 32, 1, 6, 15, 4, 22], [14, 15, 32, 1, 6, 15, 4, 22], [6, 15], [6, 15, 32], [6, 15, 32, 1], [6, 15, 32, 1, 14], [6, 15, 32, 1, 14, 15], [6, 15, 32, 1, 14, 15, 4], [6, 15, 32, 1, 14, 15, 4, 22], [6, 15, 32, 1, 14, 15, 4, 22], [14, 15], [14, 15, 32], [14, 15, 32, 1], [14, 15, 32, 1, 6], [14, 15, 32, 1, 6, 15], [14, 15, 32, 1, 6, 15, 4], [14, 15, 32, 1, 6, 15, 4, 22], [14, 15, 32, 1, 6, 15, 4, 22], [36, 37], [36, 37, 38], [36, 37, 38, 7], [36, 37, 38, 7, 9], [36, 37, 38, 7, 9, 10], [36, 37, 38, 7, 9, 10], [24, 11], [24, 11, 5], [24, 11, 5, 50], [24, 11, 5, 50, 1], [24, 11, 5, 50, 1, 2], [24, 11, 5, 50, 1, 2, 17], [24, 11, 5, 50, 1, 2, 17, 19], [24, 11, 5, 50, 1, 2, 17, 19, 11], [24, 11, 5, 50, 1, 2, 17, 19, 11, 176], [24, 11, 5, 50, 1, 2, 17, 19, 11, 176], [1, 58], [1, 58, 5], [1, 58, 5, 177], [1, 58, 5, 177, 4], [1, 58, 5, 177, 4, 178], [1, 58, 5, 177, 4, 178, 1], [1, 58, 5, 177, 4, 178, 1, 179], [1, 58, 5, 177, 4, 178, 1, 179], [20, 60], [20, 60, 180], [20, 60, 180, 181], [20, 60, 180, 181, 182], [20, 60, 180, 181, 182, 183], [20, 60, 180, 181, 182, 183, 184], [20, 60, 180, 181, 182, 183, 184], [61, 16], [61, 16, 185], [61, 16, 185, 62], [61, 16, 185, 62, 186], [61, 16, 185, 62, 186, 63], [61, 16, 185, 62, 186, 63, 187], [61, 16, 185, 62, 186, 63, 187, 188], [61, 16, 185, 62, 186, 63, 187, 188], [189, 190], [189, 190, 64], [189, 190, 64, 1], [189, 190, 64, 1, 191], [189, 190, 64, 1, 191, 192], [189, 190, 64, 1, 191, 192, 193], [189, 190, 64, 1, 191, 192, 193], [194, 7], [194, 7, 27], [194, 7, 27, 195], [194, 7, 27, 195, 1], [194, 7, 27, 195, 1, 196], [194, 7, 27, 195, 1, 196, 57], [194, 7, 27, 195, 1, 196, 57, 5], [194, 7, 27, 195, 1, 196, 57, 5], [197, 198], [197, 198, 26], [197, 198, 26, 199], [197, 198, 26, 199, 200], [197, 198, 26, 199, 200, 201], [197, 198, 26, 199, 200, 201, 202], [197, 198, 26, 199, 200, 201, 202], [20, 21], [20, 21, 203], [20, 21, 203, 204], [20, 21, 203, 204, 12], [20, 21, 203, 204, 12, 9], [20, 21, 203, 204, 12, 9, 10], [20, 21, 203, 204, 12, 9, 10], [4, 2], [4, 2, 205], [4, 2, 205, 8], [4, 2, 205, 8, 2], [4, 2, 205, 8, 2, 206], [4, 2, 205, 8, 2, 206, 63], [4, 2, 205, 8, 2, 206, 63, 207], [4, 2, 205, 8, 2, 206, 63, 207, 64], [4, 2, 205, 8, 2, 206, 63, 207, 64], [27, 208], [27, 208, 12], [27, 208, 12, 2], [27, 208, 12, 2, 209], [27, 208, 12, 2, 209, 54], [27, 208, 12, 2, 209, 54, 18], [27, 208, 12, 2, 209, 54, 18, 210], [27, 208, 12, 2, 209, 54, 18, 210, 18], [27, 208, 12, 2, 209, 54, 18, 210, 18, 3], [27, 208, 12, 2, 209, 54, 18, 210, 18, 3, 211], [27, 208, 12, 2, 209, 54, 18, 210, 18, 3, 211], [212, 8], [212, 8, 2], [212, 8, 2, 213], [212, 8, 2, 213, 214], [212, 8, 2, 213, 214, 14], [212, 8, 2, 213, 214, 14, 34], [212, 8, 2, 213, 214, 14, 34, 215], [212, 8, 2, 213, 214, 14, 34, 215], [14, 216], [14, 216, 3], [14, 216, 3, 217], [14, 216, 3, 217, 218], [14, 216, 3, 217, 218, 219], [14, 216, 3, 217, 218, 219, 220], [14, 216, 3, 217, 218, 219, 220, 6], [14, 216, 3, 217, 218, 219, 220, 6, 221], [14, 216, 3, 217, 218, 219, 220, 6, 221], [27, 222], [27, 222, 223], [27, 222, 223, 224], [27, 222, 223, 224, 225], [27, 222, 223, 224, 225, 226], [27, 222, 223, 224, 225, 226, 1], [27, 222, 223, 224, 225, 226, 1, 227], [27, 222, 223, 224, 225, 226, 1, 227], [44, 21], [44, 21, 228], [44, 21, 228, 16], [44, 21, 228, 16, 229], [44, 21, 228, 16, 229, 230], [44, 21, 228, 16, 229, 230, 231], [44, 21, 228, 16, 229, 230, 231, 32], [44, 21, 228, 16, 229, 230, 231, 32, 232], [44, 21, 228, 16, 229, 230, 231, 32, 232, 2], [44, 21, 228, 16, 229, 230, 231, 32, 232, 2, 45], [44, 21, 228, 16, 229, 230, 231, 32, 232, 2, 45], [233, 2], [233, 2, 41], [233, 2, 41, 62], [233, 2, 41, 62, 65], [233, 2, 41, 62, 65, 234], [233, 2, 41, 62, 65, 234, 2], [233, 2, 41, 62, 65, 234, 2, 235], [233, 2, 41, 62, 65, 234, 2, 235], [1, 236], [1, 236, 5], [1, 236, 5, 2], [1, 236, 5, 2, 237], [1, 236, 5, 2, 237, 12], [1, 236, 5, 2, 237, 12, 9], [1, 236, 5, 2, 237, 12, 9, 10], [1, 236, 5, 2, 237, 12, 9, 10], [24, 238], [24, 238, 24], [24, 238, 24, 239], [24, 238, 24, 239, 240], [24, 238, 24, 239, 240, 28], [24, 238, 24, 239, 240, 28, 11], [24, 238, 24, 239, 240, 28, 11, 241], [24, 238, 24, 239, 240, 28, 11, 241], [46, 23], [46, 23, 3], [46, 23, 3, 242], [46, 23, 3, 242, 65], [46, 23, 3, 242, 65, 243], [46, 23, 3, 242, 65, 243, 244], [46, 23, 3, 242, 65, 243, 244, 245], [46, 23, 3, 242, 65, 243, 244, 245], [6, 53], [6, 53, 246], [6, 53, 246, 13], [6, 53, 246, 13, 16], [6, 53, 246, 13, 16, 247], [6, 53, 246, 13, 16, 247], [1, 248], [1, 248, 31], [1, 248, 31, 3], [1, 248, 31, 3, 249], [1, 248, 31, 3, 249, 250], [1, 248, 31, 3, 249, 250], [55, 251], [55, 251, 2], [55, 251, 2, 252], [55, 251, 2, 252, 34], [55, 251, 2, 252, 34, 253], [55, 251, 2, 252, 34, 253, 254], [55, 251, 2, 252, 34, 253, 254, 255], [55, 251, 2, 252, 34, 253, 254, 255], [19, 256], [19, 256, 31], [19, 256, 31, 16], [19, 256, 31, 16, 257], [19, 256, 31, 16, 257, 258], [19, 256, 31, 16, 257, 258, 259], [19, 256, 31, 16, 257, 258, 259, 1], [19, 256, 31, 16, 257, 258, 259, 1, 5], [19, 256, 31, 16, 257, 258, 259, 1, 5], [2, 17], [2, 17, 4], [2, 17, 4, 56], [2, 17, 4, 56, 260], [2, 17, 4, 56, 260, 19], [2, 17, 4, 56, 260, 19, 23], [2, 17, 4, 56, 260, 19, 23, 5], [2, 17, 4, 56, 260, 19, 23, 5, 261], [2, 17, 4, 56, 260, 19, 23, 5, 261], [1, 26], [1, 26, 61], [1, 26, 61, 60], [1, 26, 61, 60, 262], [1, 26, 61, 60, 262, 13], [1, 26, 61, 60, 262, 13, 9], [1, 26, 61, 60, 262, 13, 9, 10], [1, 26, 61, 60, 262, 13, 9, 10]]\n",
            "size of training set: 517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2avqaGTFryX"
      },
      "source": [
        "## Find out the longest length of sequence in `input_sequences`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATXAEiT6FKwb",
        "outputId": "1fdc7acb-35e3-41d2-936c-11d71312fd10"
      },
      "source": [
        "max_sequences_len = max([len(x) for x in input_sequences])\n",
        "print('max length of the sequences: {}'.format(max_sequences_len))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max length of the sequences: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZNRDS2gIhwY"
      },
      "source": [
        "## Use `pad_sequences` to pad all the sequences, so they are the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8ZKrQgtGDxu"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(input_sequences, maxlen=max_sequences_len, padding='pre')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCmDw9R1JUTE"
      },
      "source": [
        "## Convert the padded list to the numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL88VRKgJKSf",
        "outputId": "0451ffcd-9b1d-4afe-c7d2-3a78a619c50c"
      },
      "source": [
        "import numpy as np\n",
        "padded = np.array(padded)\n",
        "print(padded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0 ...   0   4   2]\n",
            " [  0   0   0 ...   4   2  66]\n",
            " [  0   0   0 ...   2  66   8]\n",
            " ...\n",
            " [  0   0   0 ... 262  13   9]\n",
            " [  0   0   0 ...  13   9  10]\n",
            " [  0   0   0 ...  13   9  10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ_f4_SUKgF5"
      },
      "source": [
        "## Create `xs` or predictors and labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXS07Z-qLUPD",
        "outputId": "dbe8a8a2-25e0-4d94-a71d-cbd7c6bb9ad6"
      },
      "source": [
        "example = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]], np.int32)\n",
        "print('example: \\n{} \\n'.format(example))\n",
        "\n",
        "x = example[:,:-1]\n",
        "print('x: \\n{}\\n'.format(x))\n",
        "\n",
        "y = example[:,-1]\n",
        "print('y: \\n{} \\n'.format(y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example: \n",
            "[[ 1  2  3  4  5  6]\n",
            " [ 7  8  9 10 11 12]] \n",
            "\n",
            "x: \n",
            "[[ 1  2  3  4  5]\n",
            " [ 7  8  9 10 11]]\n",
            "\n",
            "y: \n",
            "[ 6 12] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAn39KVYJfuH"
      },
      "source": [
        "xs = padded[:,:-1]\n",
        "labels = padded[:,-1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZBinQOYK3_m",
        "outputId": "dbc9423d-b417-4ad4-a068-b2f73620438c"
      },
      "source": [
        "print(padded[0])\n",
        "print(xs[0])\n",
        "print(labels[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 4 2]\n",
            "[0 0 0 0 0 0 0 0 0 4]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SzN8FzrK7dn",
        "outputId": "f1a4523d-6c01-4832-95de-d1a379f96be1"
      },
      "source": [
        "print(padded[1])\n",
        "print(xs[1])\n",
        "print(labels[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  0  4  2 66]\n",
            "[0 0 0 0 0 0 0 0 4 2]\n",
            "66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_wsJeoKLPiV",
        "outputId": "5f3135d4-538e-46d5-9cac-62d58185dffc"
      },
      "source": [
        "print(padded[2])\n",
        "print(xs[2])\n",
        "print(labels[2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  4  2 66  8]\n",
            "[ 0  0  0  0  0  0  0  4  2 66]\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7171l_yO3_S"
      },
      "source": [
        "## one-hot encode the labels as this is a classification problem: \n",
        "use the contrast utility to convert a list to a categorical. Simply give the list of labels and the number of classes which is total number of words, and it will create a one-hot encoding of the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwKXBg00MgsZ",
        "outputId": "08f139e0-1107-4560-f6c4-c05aa5826be4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "print(ys)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTgGCnxzPIkx",
        "outputId": "74b63079-3fe6-46b6-a054-f09212ecbe41"
      },
      "source": [
        "print('the first sequence: \\n{}\\n'.format(padded[0]))\n",
        "print('xs[0]: \\n{}\\n'.format(xs[0]))\n",
        "print('the label of xs[0]: \\n{}\\n'.format(labels[0]))\n",
        "print('one hot representation of the label: \\n{}\\n'.format(ys))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the first sequence: \n",
            "[0 0 0 0 0 0 0 0 0 4 2]\n",
            "\n",
            "xs[0]: \n",
            "[0 0 0 0 0 0 0 0 0 4]\n",
            "\n",
            "the label of xs[0]: \n",
            "2\n",
            "\n",
            "one hot representation of the label: \n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y--rhVgTUq_t"
      },
      "source": [
        "## Let's look at another example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_fsY6TJQM-n",
        "outputId": "13450a5b-76ea-45bc-8143-d9e7833abdf7"
      },
      "source": [
        "print('Example padded sentence: \\n{}\\n'.format(padded[6]))\n",
        "original_sentence = ''\n",
        "for index in padded[6]:\n",
        "  for key, value in word_index.items():\n",
        "    if value==index:\n",
        "      original_sentence+= key + ' '\n",
        "print('Original sentence was: \\n{}\\n'.format(original_sentence))\n",
        "print('xs: \\n{}\\n'.format(xs[6]))\n",
        "print('Label: \\n{}\\n'.format(labels[6]))\n",
        "print('ys: \\n{}\\n'.format(ys[6]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example padded sentence: \n",
            "[ 0  0  0  4  2 66  8 67 68 69 70]\n",
            "\n",
            "Original sentence was: \n",
            "in the town of athy one jeremy lanigan \n",
            "\n",
            "xs: \n",
            "[ 0  0  0  4  2 66  8 67 68 69]\n",
            "\n",
            "Label: \n",
            "70\n",
            "\n",
            "ys: \n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv6yuAcOdnsm"
      },
      "source": [
        "## Summary:\n",
        "We start with a data, a string containing a single song, and prepare that for generating new text by tokenizing the data and then create sub-sentence engrams that were labelled with the next word in the sentence and then one-hot encoded the labels.\n",
        "\n",
        "Now we can build a neural network that can, given a sentence, predict the next word. The data is xs and ys, and it's relatively simple to create a neural network to classify what the next word should be, given a set of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WR-LzwLZ_I3"
      },
      "source": [
        "## Build a LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u98VCS_eU18e",
        "outputId": "961c729f-a701-4c9b-9ef1-9039374bd113"
      },
      "source": [
        "  model = tf.keras.models.Sequential([tf.keras.layers.Embedding(total_words, 64, input_length=max_sequences_len-1),\n",
        "                                      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
        "                                      tf.keras.layers.Dense(total_words, activation='softmax')])\n",
        "  ### Since we want to handle all words, thus in embedding layer, is set to total_words\n",
        "  \n",
        "  ### input_length: since we cropped off the last word of each sequence to get label, then\n",
        "  ### the sequence will be one less than the maximum\n",
        "\n",
        "  ### The last dense layer sized as the total words, which is the same size that is used for\n",
        "  ### the one-hot encoding. Thus this layer will have one neuron, per word and that neuron \n",
        "  ### should light up when we predict a given word.\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(xs, ys, epochs=500, verbose=2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "17/17 - 9s - loss: 5.5680 - accuracy: 0.0348\n",
            "Epoch 2/500\n",
            "17/17 - 0s - loss: 5.5403 - accuracy: 0.0406\n",
            "Epoch 3/500\n",
            "17/17 - 0s - loss: 5.4720 - accuracy: 0.0387\n",
            "Epoch 4/500\n",
            "17/17 - 0s - loss: 5.2748 - accuracy: 0.0348\n",
            "Epoch 5/500\n",
            "17/17 - 0s - loss: 5.1271 - accuracy: 0.0426\n",
            "Epoch 6/500\n",
            "17/17 - 0s - loss: 5.0565 - accuracy: 0.0445\n",
            "Epoch 7/500\n",
            "17/17 - 0s - loss: 4.9962 - accuracy: 0.0503\n",
            "Epoch 8/500\n",
            "17/17 - 0s - loss: 4.9393 - accuracy: 0.1025\n",
            "Epoch 9/500\n",
            "17/17 - 0s - loss: 4.8791 - accuracy: 0.1006\n",
            "Epoch 10/500\n",
            "17/17 - 0s - loss: 4.8159 - accuracy: 0.0928\n",
            "Epoch 11/500\n",
            "17/17 - 0s - loss: 4.7453 - accuracy: 0.1006\n",
            "Epoch 12/500\n",
            "17/17 - 0s - loss: 4.6749 - accuracy: 0.0909\n",
            "Epoch 13/500\n",
            "17/17 - 0s - loss: 4.5890 - accuracy: 0.1103\n",
            "Epoch 14/500\n",
            "17/17 - 0s - loss: 4.5135 - accuracy: 0.1161\n",
            "Epoch 15/500\n",
            "17/17 - 0s - loss: 4.4434 - accuracy: 0.1064\n",
            "Epoch 16/500\n",
            "17/17 - 0s - loss: 4.3649 - accuracy: 0.1238\n",
            "Epoch 17/500\n",
            "17/17 - 0s - loss: 4.2948 - accuracy: 0.1451\n",
            "Epoch 18/500\n",
            "17/17 - 0s - loss: 4.2270 - accuracy: 0.1702\n",
            "Epoch 19/500\n",
            "17/17 - 0s - loss: 4.1502 - accuracy: 0.1702\n",
            "Epoch 20/500\n",
            "17/17 - 0s - loss: 4.0903 - accuracy: 0.1760\n",
            "Epoch 21/500\n",
            "17/17 - 0s - loss: 4.0329 - accuracy: 0.1857\n",
            "Epoch 22/500\n",
            "17/17 - 0s - loss: 3.9640 - accuracy: 0.2031\n",
            "Epoch 23/500\n",
            "17/17 - 0s - loss: 3.9023 - accuracy: 0.2166\n",
            "Epoch 24/500\n",
            "17/17 - 0s - loss: 3.8400 - accuracy: 0.2108\n",
            "Epoch 25/500\n",
            "17/17 - 0s - loss: 3.7802 - accuracy: 0.2128\n",
            "Epoch 26/500\n",
            "17/17 - 0s - loss: 3.7367 - accuracy: 0.2166\n",
            "Epoch 27/500\n",
            "17/17 - 0s - loss: 3.6890 - accuracy: 0.2263\n",
            "Epoch 28/500\n",
            "17/17 - 0s - loss: 3.6221 - accuracy: 0.2515\n",
            "Epoch 29/500\n",
            "17/17 - 0s - loss: 3.5617 - accuracy: 0.2573\n",
            "Epoch 30/500\n",
            "17/17 - 0s - loss: 3.4998 - accuracy: 0.2689\n",
            "Epoch 31/500\n",
            "17/17 - 0s - loss: 3.4479 - accuracy: 0.2882\n",
            "Epoch 32/500\n",
            "17/17 - 0s - loss: 3.3879 - accuracy: 0.2805\n",
            "Epoch 33/500\n",
            "17/17 - 0s - loss: 3.3332 - accuracy: 0.3075\n",
            "Epoch 34/500\n",
            "17/17 - 0s - loss: 3.2854 - accuracy: 0.2979\n",
            "Epoch 35/500\n",
            "17/17 - 0s - loss: 3.2392 - accuracy: 0.3056\n",
            "Epoch 36/500\n",
            "17/17 - 0s - loss: 3.2103 - accuracy: 0.3037\n",
            "Epoch 37/500\n",
            "17/17 - 0s - loss: 3.1540 - accuracy: 0.3191\n",
            "Epoch 38/500\n",
            "17/17 - 0s - loss: 3.1022 - accuracy: 0.3308\n",
            "Epoch 39/500\n",
            "17/17 - 0s - loss: 3.0679 - accuracy: 0.3269\n",
            "Epoch 40/500\n",
            "17/17 - 0s - loss: 3.0439 - accuracy: 0.3424\n",
            "Epoch 41/500\n",
            "17/17 - 0s - loss: 2.9894 - accuracy: 0.3443\n",
            "Epoch 42/500\n",
            "17/17 - 0s - loss: 2.9470 - accuracy: 0.3520\n",
            "Epoch 43/500\n",
            "17/17 - 0s - loss: 2.9161 - accuracy: 0.3501\n",
            "Epoch 44/500\n",
            "17/17 - 0s - loss: 2.8604 - accuracy: 0.3733\n",
            "Epoch 45/500\n",
            "17/17 - 0s - loss: 2.8247 - accuracy: 0.3926\n",
            "Epoch 46/500\n",
            "17/17 - 0s - loss: 2.7697 - accuracy: 0.4004\n",
            "Epoch 47/500\n",
            "17/17 - 0s - loss: 2.7277 - accuracy: 0.4081\n",
            "Epoch 48/500\n",
            "17/17 - 0s - loss: 2.6929 - accuracy: 0.4120\n",
            "Epoch 49/500\n",
            "17/17 - 0s - loss: 2.6687 - accuracy: 0.4313\n",
            "Epoch 50/500\n",
            "17/17 - 0s - loss: 2.6312 - accuracy: 0.4255\n",
            "Epoch 51/500\n",
            "17/17 - 0s - loss: 2.5910 - accuracy: 0.4294\n",
            "Epoch 52/500\n",
            "17/17 - 0s - loss: 2.5473 - accuracy: 0.4642\n",
            "Epoch 53/500\n",
            "17/17 - 0s - loss: 2.5184 - accuracy: 0.4507\n",
            "Epoch 54/500\n",
            "17/17 - 0s - loss: 2.5019 - accuracy: 0.4720\n",
            "Epoch 55/500\n",
            "17/17 - 0s - loss: 2.4692 - accuracy: 0.4778\n",
            "Epoch 56/500\n",
            "17/17 - 0s - loss: 2.4616 - accuracy: 0.4642\n",
            "Epoch 57/500\n",
            "17/17 - 0s - loss: 2.4267 - accuracy: 0.4836\n",
            "Epoch 58/500\n",
            "17/17 - 0s - loss: 2.3942 - accuracy: 0.4990\n",
            "Epoch 59/500\n",
            "17/17 - 0s - loss: 2.3617 - accuracy: 0.5087\n",
            "Epoch 60/500\n",
            "17/17 - 0s - loss: 2.3247 - accuracy: 0.5087\n",
            "Epoch 61/500\n",
            "17/17 - 0s - loss: 2.3025 - accuracy: 0.5145\n",
            "Epoch 62/500\n",
            "17/17 - 0s - loss: 2.2713 - accuracy: 0.5397\n",
            "Epoch 63/500\n",
            "17/17 - 0s - loss: 2.2381 - accuracy: 0.5435\n",
            "Epoch 64/500\n",
            "17/17 - 0s - loss: 2.2127 - accuracy: 0.5455\n",
            "Epoch 65/500\n",
            "17/17 - 0s - loss: 2.1833 - accuracy: 0.5513\n",
            "Epoch 66/500\n",
            "17/17 - 0s - loss: 2.1591 - accuracy: 0.5513\n",
            "Epoch 67/500\n",
            "17/17 - 0s - loss: 2.1350 - accuracy: 0.5513\n",
            "Epoch 68/500\n",
            "17/17 - 0s - loss: 2.1129 - accuracy: 0.5687\n",
            "Epoch 69/500\n",
            "17/17 - 0s - loss: 2.0874 - accuracy: 0.5745\n",
            "Epoch 70/500\n",
            "17/17 - 0s - loss: 2.0690 - accuracy: 0.5764\n",
            "Epoch 71/500\n",
            "17/17 - 0s - loss: 2.0480 - accuracy: 0.5899\n",
            "Epoch 72/500\n",
            "17/17 - 0s - loss: 2.0198 - accuracy: 0.6015\n",
            "Epoch 73/500\n",
            "17/17 - 0s - loss: 2.0031 - accuracy: 0.6035\n",
            "Epoch 74/500\n",
            "17/17 - 0s - loss: 1.9912 - accuracy: 0.6112\n",
            "Epoch 75/500\n",
            "17/17 - 0s - loss: 1.9706 - accuracy: 0.6054\n",
            "Epoch 76/500\n",
            "17/17 - 0s - loss: 1.9410 - accuracy: 0.6093\n",
            "Epoch 77/500\n",
            "17/17 - 0s - loss: 1.9152 - accuracy: 0.6286\n",
            "Epoch 78/500\n",
            "17/17 - 0s - loss: 1.8955 - accuracy: 0.6402\n",
            "Epoch 79/500\n",
            "17/17 - 0s - loss: 1.8811 - accuracy: 0.6364\n",
            "Epoch 80/500\n",
            "17/17 - 0s - loss: 1.8518 - accuracy: 0.6596\n",
            "Epoch 81/500\n",
            "17/17 - 0s - loss: 1.8300 - accuracy: 0.6518\n",
            "Epoch 82/500\n",
            "17/17 - 0s - loss: 1.8100 - accuracy: 0.6634\n",
            "Epoch 83/500\n",
            "17/17 - 0s - loss: 1.7916 - accuracy: 0.6712\n",
            "Epoch 84/500\n",
            "17/17 - 0s - loss: 1.7640 - accuracy: 0.6809\n",
            "Epoch 85/500\n",
            "17/17 - 0s - loss: 1.7441 - accuracy: 0.6731\n",
            "Epoch 86/500\n",
            "17/17 - 0s - loss: 1.7206 - accuracy: 0.6770\n",
            "Epoch 87/500\n",
            "17/17 - 0s - loss: 1.7062 - accuracy: 0.6731\n",
            "Epoch 88/500\n",
            "17/17 - 0s - loss: 1.6897 - accuracy: 0.6828\n",
            "Epoch 89/500\n",
            "17/17 - 0s - loss: 1.6704 - accuracy: 0.6963\n",
            "Epoch 90/500\n",
            "17/17 - 0s - loss: 1.6490 - accuracy: 0.7021\n",
            "Epoch 91/500\n",
            "17/17 - 0s - loss: 1.6334 - accuracy: 0.7195\n",
            "Epoch 92/500\n",
            "17/17 - 0s - loss: 1.6308 - accuracy: 0.7137\n",
            "Epoch 93/500\n",
            "17/17 - 0s - loss: 1.6153 - accuracy: 0.7176\n",
            "Epoch 94/500\n",
            "17/17 - 0s - loss: 1.5973 - accuracy: 0.7176\n",
            "Epoch 95/500\n",
            "17/17 - 0s - loss: 1.6109 - accuracy: 0.7099\n",
            "Epoch 96/500\n",
            "17/17 - 0s - loss: 1.5850 - accuracy: 0.7176\n",
            "Epoch 97/500\n",
            "17/17 - 0s - loss: 1.5610 - accuracy: 0.7273\n",
            "Epoch 98/500\n",
            "17/17 - 0s - loss: 1.5417 - accuracy: 0.7350\n",
            "Epoch 99/500\n",
            "17/17 - 0s - loss: 1.5293 - accuracy: 0.7292\n",
            "Epoch 100/500\n",
            "17/17 - 0s - loss: 1.5168 - accuracy: 0.7350\n",
            "Epoch 101/500\n",
            "17/17 - 0s - loss: 1.5301 - accuracy: 0.7311\n",
            "Epoch 102/500\n",
            "17/17 - 0s - loss: 1.5081 - accuracy: 0.7350\n",
            "Epoch 103/500\n",
            "17/17 - 0s - loss: 1.5410 - accuracy: 0.7137\n",
            "Epoch 104/500\n",
            "17/17 - 0s - loss: 1.4921 - accuracy: 0.7408\n",
            "Epoch 105/500\n",
            "17/17 - 0s - loss: 1.4585 - accuracy: 0.7427\n",
            "Epoch 106/500\n",
            "17/17 - 0s - loss: 1.4357 - accuracy: 0.7563\n",
            "Epoch 107/500\n",
            "17/17 - 0s - loss: 1.4257 - accuracy: 0.7524\n",
            "Epoch 108/500\n",
            "17/17 - 0s - loss: 1.4047 - accuracy: 0.7485\n",
            "Epoch 109/500\n",
            "17/17 - 0s - loss: 1.3915 - accuracy: 0.7544\n",
            "Epoch 110/500\n",
            "17/17 - 0s - loss: 1.3800 - accuracy: 0.7544\n",
            "Epoch 111/500\n",
            "17/17 - 0s - loss: 1.3573 - accuracy: 0.7679\n",
            "Epoch 112/500\n",
            "17/17 - 0s - loss: 1.3410 - accuracy: 0.7640\n",
            "Epoch 113/500\n",
            "17/17 - 0s - loss: 1.3234 - accuracy: 0.7756\n",
            "Epoch 114/500\n",
            "17/17 - 0s - loss: 1.3092 - accuracy: 0.7776\n",
            "Epoch 115/500\n",
            "17/17 - 0s - loss: 1.2925 - accuracy: 0.7834\n",
            "Epoch 116/500\n",
            "17/17 - 0s - loss: 1.2791 - accuracy: 0.7756\n",
            "Epoch 117/500\n",
            "17/17 - 0s - loss: 1.2671 - accuracy: 0.7814\n",
            "Epoch 118/500\n",
            "17/17 - 0s - loss: 1.2581 - accuracy: 0.7834\n",
            "Epoch 119/500\n",
            "17/17 - 0s - loss: 1.2458 - accuracy: 0.7834\n",
            "Epoch 120/500\n",
            "17/17 - 0s - loss: 1.2334 - accuracy: 0.7853\n",
            "Epoch 121/500\n",
            "17/17 - 0s - loss: 1.2192 - accuracy: 0.7872\n",
            "Epoch 122/500\n",
            "17/17 - 0s - loss: 1.2035 - accuracy: 0.7853\n",
            "Epoch 123/500\n",
            "17/17 - 0s - loss: 1.1920 - accuracy: 0.7969\n",
            "Epoch 124/500\n",
            "17/17 - 0s - loss: 1.1730 - accuracy: 0.8027\n",
            "Epoch 125/500\n",
            "17/17 - 0s - loss: 1.1701 - accuracy: 0.7988\n",
            "Epoch 126/500\n",
            "17/17 - 0s - loss: 1.1580 - accuracy: 0.8066\n",
            "Epoch 127/500\n",
            "17/17 - 0s - loss: 1.1399 - accuracy: 0.8104\n",
            "Epoch 128/500\n",
            "17/17 - 0s - loss: 1.1281 - accuracy: 0.8162\n",
            "Epoch 129/500\n",
            "17/17 - 0s - loss: 1.1162 - accuracy: 0.8201\n",
            "Epoch 130/500\n",
            "17/17 - 0s - loss: 1.1069 - accuracy: 0.8259\n",
            "Epoch 131/500\n",
            "17/17 - 0s - loss: 1.1001 - accuracy: 0.8221\n",
            "Epoch 132/500\n",
            "17/17 - 0s - loss: 1.1124 - accuracy: 0.8221\n",
            "Epoch 133/500\n",
            "17/17 - 0s - loss: 1.0984 - accuracy: 0.8279\n",
            "Epoch 134/500\n",
            "17/17 - 0s - loss: 1.0914 - accuracy: 0.8259\n",
            "Epoch 135/500\n",
            "17/17 - 0s - loss: 1.1178 - accuracy: 0.8104\n",
            "Epoch 136/500\n",
            "17/17 - 0s - loss: 1.0931 - accuracy: 0.8279\n",
            "Epoch 137/500\n",
            "17/17 - 0s - loss: 1.0621 - accuracy: 0.8259\n",
            "Epoch 138/500\n",
            "17/17 - 0s - loss: 1.0476 - accuracy: 0.8279\n",
            "Epoch 139/500\n",
            "17/17 - 0s - loss: 1.0428 - accuracy: 0.8317\n",
            "Epoch 140/500\n",
            "17/17 - 0s - loss: 1.0337 - accuracy: 0.8298\n",
            "Epoch 141/500\n",
            "17/17 - 0s - loss: 1.0142 - accuracy: 0.8395\n",
            "Epoch 142/500\n",
            "17/17 - 0s - loss: 1.0091 - accuracy: 0.8414\n",
            "Epoch 143/500\n",
            "17/17 - 0s - loss: 0.9888 - accuracy: 0.8472\n",
            "Epoch 144/500\n",
            "17/17 - 0s - loss: 0.9798 - accuracy: 0.8491\n",
            "Epoch 145/500\n",
            "17/17 - 0s - loss: 0.9660 - accuracy: 0.8511\n",
            "Epoch 146/500\n",
            "17/17 - 0s - loss: 0.9570 - accuracy: 0.8530\n",
            "Epoch 147/500\n",
            "17/17 - 0s - loss: 0.9402 - accuracy: 0.8607\n",
            "Epoch 148/500\n",
            "17/17 - 0s - loss: 0.9220 - accuracy: 0.8665\n",
            "Epoch 149/500\n",
            "17/17 - 0s - loss: 0.9121 - accuracy: 0.8646\n",
            "Epoch 150/500\n",
            "17/17 - 0s - loss: 0.9038 - accuracy: 0.8704\n",
            "Epoch 151/500\n",
            "17/17 - 0s - loss: 0.9050 - accuracy: 0.8588\n",
            "Epoch 152/500\n",
            "17/17 - 0s - loss: 0.8989 - accuracy: 0.8646\n",
            "Epoch 153/500\n",
            "17/17 - 0s - loss: 0.8864 - accuracy: 0.8743\n",
            "Epoch 154/500\n",
            "17/17 - 0s - loss: 0.8743 - accuracy: 0.8646\n",
            "Epoch 155/500\n",
            "17/17 - 0s - loss: 0.8983 - accuracy: 0.8569\n",
            "Epoch 156/500\n",
            "17/17 - 0s - loss: 0.9014 - accuracy: 0.8569\n",
            "Epoch 157/500\n",
            "17/17 - 0s - loss: 0.8811 - accuracy: 0.8549\n",
            "Epoch 158/500\n",
            "17/17 - 0s - loss: 0.8593 - accuracy: 0.8704\n",
            "Epoch 159/500\n",
            "17/17 - 0s - loss: 0.8585 - accuracy: 0.8588\n",
            "Epoch 160/500\n",
            "17/17 - 0s - loss: 0.8311 - accuracy: 0.8704\n",
            "Epoch 161/500\n",
            "17/17 - 0s - loss: 0.8211 - accuracy: 0.8723\n",
            "Epoch 162/500\n",
            "17/17 - 0s - loss: 0.8085 - accuracy: 0.8801\n",
            "Epoch 163/500\n",
            "17/17 - 0s - loss: 0.7993 - accuracy: 0.8820\n",
            "Epoch 164/500\n",
            "17/17 - 0s - loss: 0.7903 - accuracy: 0.8839\n",
            "Epoch 165/500\n",
            "17/17 - 0s - loss: 0.7803 - accuracy: 0.8820\n",
            "Epoch 166/500\n",
            "17/17 - 0s - loss: 0.7728 - accuracy: 0.8897\n",
            "Epoch 167/500\n",
            "17/17 - 0s - loss: 0.7654 - accuracy: 0.8859\n",
            "Epoch 168/500\n",
            "17/17 - 0s - loss: 0.7567 - accuracy: 0.8936\n",
            "Epoch 169/500\n",
            "17/17 - 0s - loss: 0.7482 - accuracy: 0.8936\n",
            "Epoch 170/500\n",
            "17/17 - 0s - loss: 0.7411 - accuracy: 0.8975\n",
            "Epoch 171/500\n",
            "17/17 - 0s - loss: 0.7338 - accuracy: 0.8994\n",
            "Epoch 172/500\n",
            "17/17 - 0s - loss: 0.7271 - accuracy: 0.9014\n",
            "Epoch 173/500\n",
            "17/17 - 0s - loss: 0.7189 - accuracy: 0.9014\n",
            "Epoch 174/500\n",
            "17/17 - 0s - loss: 0.7105 - accuracy: 0.9091\n",
            "Epoch 175/500\n",
            "17/17 - 0s - loss: 0.7041 - accuracy: 0.9052\n",
            "Epoch 176/500\n",
            "17/17 - 0s - loss: 0.7006 - accuracy: 0.9033\n",
            "Epoch 177/500\n",
            "17/17 - 0s - loss: 0.6932 - accuracy: 0.9072\n",
            "Epoch 178/500\n",
            "17/17 - 0s - loss: 0.6840 - accuracy: 0.9072\n",
            "Epoch 179/500\n",
            "17/17 - 0s - loss: 0.6775 - accuracy: 0.9072\n",
            "Epoch 180/500\n",
            "17/17 - 0s - loss: 0.6699 - accuracy: 0.9052\n",
            "Epoch 181/500\n",
            "17/17 - 0s - loss: 0.6631 - accuracy: 0.9091\n",
            "Epoch 182/500\n",
            "17/17 - 0s - loss: 0.6563 - accuracy: 0.9110\n",
            "Epoch 183/500\n",
            "17/17 - 0s - loss: 0.6476 - accuracy: 0.9149\n",
            "Epoch 184/500\n",
            "17/17 - 0s - loss: 0.6433 - accuracy: 0.9149\n",
            "Epoch 185/500\n",
            "17/17 - 0s - loss: 0.6395 - accuracy: 0.9149\n",
            "Epoch 186/500\n",
            "17/17 - 0s - loss: 0.6332 - accuracy: 0.9110\n",
            "Epoch 187/500\n",
            "17/17 - 0s - loss: 0.6273 - accuracy: 0.9149\n",
            "Epoch 188/500\n",
            "17/17 - 0s - loss: 0.6266 - accuracy: 0.9072\n",
            "Epoch 189/500\n",
            "17/17 - 0s - loss: 0.6209 - accuracy: 0.9091\n",
            "Epoch 190/500\n",
            "17/17 - 0s - loss: 0.6194 - accuracy: 0.9110\n",
            "Epoch 191/500\n",
            "17/17 - 0s - loss: 0.6301 - accuracy: 0.8975\n",
            "Epoch 192/500\n",
            "17/17 - 0s - loss: 0.6565 - accuracy: 0.8897\n",
            "Epoch 193/500\n",
            "17/17 - 0s - loss: 0.6522 - accuracy: 0.9014\n",
            "Epoch 194/500\n",
            "17/17 - 0s - loss: 0.6551 - accuracy: 0.8975\n",
            "Epoch 195/500\n",
            "17/17 - 0s - loss: 0.6898 - accuracy: 0.8743\n",
            "Epoch 196/500\n",
            "17/17 - 0s - loss: 0.6661 - accuracy: 0.8878\n",
            "Epoch 197/500\n",
            "17/17 - 0s - loss: 0.6444 - accuracy: 0.8917\n",
            "Epoch 198/500\n",
            "17/17 - 0s - loss: 0.6639 - accuracy: 0.8820\n",
            "Epoch 199/500\n",
            "17/17 - 0s - loss: 0.6467 - accuracy: 0.8917\n",
            "Epoch 200/500\n",
            "17/17 - 0s - loss: 0.6347 - accuracy: 0.8956\n",
            "Epoch 201/500\n",
            "17/17 - 0s - loss: 0.5988 - accuracy: 0.9052\n",
            "Epoch 202/500\n",
            "17/17 - 0s - loss: 0.5883 - accuracy: 0.9149\n",
            "Epoch 203/500\n",
            "17/17 - 0s - loss: 0.5781 - accuracy: 0.9072\n",
            "Epoch 204/500\n",
            "17/17 - 0s - loss: 0.5621 - accuracy: 0.9149\n",
            "Epoch 205/500\n",
            "17/17 - 0s - loss: 0.5595 - accuracy: 0.9110\n",
            "Epoch 206/500\n",
            "17/17 - 0s - loss: 0.5457 - accuracy: 0.9149\n",
            "Epoch 207/500\n",
            "17/17 - 0s - loss: 0.5465 - accuracy: 0.9149\n",
            "Epoch 208/500\n",
            "17/17 - 0s - loss: 0.5356 - accuracy: 0.9226\n",
            "Epoch 209/500\n",
            "17/17 - 0s - loss: 0.5274 - accuracy: 0.9226\n",
            "Epoch 210/500\n",
            "17/17 - 0s - loss: 0.5205 - accuracy: 0.9226\n",
            "Epoch 211/500\n",
            "17/17 - 0s - loss: 0.5182 - accuracy: 0.9226\n",
            "Epoch 212/500\n",
            "17/17 - 0s - loss: 0.5103 - accuracy: 0.9207\n",
            "Epoch 213/500\n",
            "17/17 - 0s - loss: 0.5023 - accuracy: 0.9246\n",
            "Epoch 214/500\n",
            "17/17 - 0s - loss: 0.4975 - accuracy: 0.9207\n",
            "Epoch 215/500\n",
            "17/17 - 0s - loss: 0.4934 - accuracy: 0.9265\n",
            "Epoch 216/500\n",
            "17/17 - 0s - loss: 0.4890 - accuracy: 0.9284\n",
            "Epoch 217/500\n",
            "17/17 - 0s - loss: 0.4838 - accuracy: 0.9284\n",
            "Epoch 218/500\n",
            "17/17 - 0s - loss: 0.4780 - accuracy: 0.9284\n",
            "Epoch 219/500\n",
            "17/17 - 0s - loss: 0.4742 - accuracy: 0.9284\n",
            "Epoch 220/500\n",
            "17/17 - 0s - loss: 0.4721 - accuracy: 0.9304\n",
            "Epoch 221/500\n",
            "17/17 - 0s - loss: 0.4619 - accuracy: 0.9304\n",
            "Epoch 222/500\n",
            "17/17 - 0s - loss: 0.4582 - accuracy: 0.9304\n",
            "Epoch 223/500\n",
            "17/17 - 0s - loss: 0.4526 - accuracy: 0.9284\n",
            "Epoch 224/500\n",
            "17/17 - 0s - loss: 0.4471 - accuracy: 0.9323\n",
            "Epoch 225/500\n",
            "17/17 - 0s - loss: 0.4436 - accuracy: 0.9304\n",
            "Epoch 226/500\n",
            "17/17 - 0s - loss: 0.4383 - accuracy: 0.9323\n",
            "Epoch 227/500\n",
            "17/17 - 0s - loss: 0.4338 - accuracy: 0.9323\n",
            "Epoch 228/500\n",
            "17/17 - 0s - loss: 0.4297 - accuracy: 0.9323\n",
            "Epoch 229/500\n",
            "17/17 - 0s - loss: 0.4279 - accuracy: 0.9304\n",
            "Epoch 230/500\n",
            "17/17 - 0s - loss: 0.4235 - accuracy: 0.9323\n",
            "Epoch 231/500\n",
            "17/17 - 0s - loss: 0.4182 - accuracy: 0.9304\n",
            "Epoch 232/500\n",
            "17/17 - 0s - loss: 0.4148 - accuracy: 0.9304\n",
            "Epoch 233/500\n",
            "17/17 - 0s - loss: 0.4107 - accuracy: 0.9342\n",
            "Epoch 234/500\n",
            "17/17 - 0s - loss: 0.4065 - accuracy: 0.9323\n",
            "Epoch 235/500\n",
            "17/17 - 0s - loss: 0.4020 - accuracy: 0.9323\n",
            "Epoch 236/500\n",
            "17/17 - 0s - loss: 0.3988 - accuracy: 0.9323\n",
            "Epoch 237/500\n",
            "17/17 - 0s - loss: 0.3964 - accuracy: 0.9362\n",
            "Epoch 238/500\n",
            "17/17 - 0s - loss: 0.3938 - accuracy: 0.9342\n",
            "Epoch 239/500\n",
            "17/17 - 0s - loss: 0.3896 - accuracy: 0.9362\n",
            "Epoch 240/500\n",
            "17/17 - 0s - loss: 0.3853 - accuracy: 0.9362\n",
            "Epoch 241/500\n",
            "17/17 - 0s - loss: 0.3833 - accuracy: 0.9362\n",
            "Epoch 242/500\n",
            "17/17 - 0s - loss: 0.3786 - accuracy: 0.9381\n",
            "Epoch 243/500\n",
            "17/17 - 0s - loss: 0.3774 - accuracy: 0.9381\n",
            "Epoch 244/500\n",
            "17/17 - 0s - loss: 0.3736 - accuracy: 0.9362\n",
            "Epoch 245/500\n",
            "17/17 - 0s - loss: 0.3700 - accuracy: 0.9381\n",
            "Epoch 246/500\n",
            "17/17 - 0s - loss: 0.3668 - accuracy: 0.9362\n",
            "Epoch 247/500\n",
            "17/17 - 0s - loss: 0.3632 - accuracy: 0.9381\n",
            "Epoch 248/500\n",
            "17/17 - 0s - loss: 0.3604 - accuracy: 0.9381\n",
            "Epoch 249/500\n",
            "17/17 - 0s - loss: 0.3570 - accuracy: 0.9439\n",
            "Epoch 250/500\n",
            "17/17 - 0s - loss: 0.3541 - accuracy: 0.9439\n",
            "Epoch 251/500\n",
            "17/17 - 0s - loss: 0.3511 - accuracy: 0.9420\n",
            "Epoch 252/500\n",
            "17/17 - 0s - loss: 0.3479 - accuracy: 0.9381\n",
            "Epoch 253/500\n",
            "17/17 - 0s - loss: 0.3449 - accuracy: 0.9420\n",
            "Epoch 254/500\n",
            "17/17 - 0s - loss: 0.3417 - accuracy: 0.9400\n",
            "Epoch 255/500\n",
            "17/17 - 0s - loss: 0.3393 - accuracy: 0.9420\n",
            "Epoch 256/500\n",
            "17/17 - 0s - loss: 0.3387 - accuracy: 0.9420\n",
            "Epoch 257/500\n",
            "17/17 - 0s - loss: 0.3410 - accuracy: 0.9420\n",
            "Epoch 258/500\n",
            "17/17 - 0s - loss: 0.3394 - accuracy: 0.9439\n",
            "Epoch 259/500\n",
            "17/17 - 0s - loss: 0.3403 - accuracy: 0.9400\n",
            "Epoch 260/500\n",
            "17/17 - 0s - loss: 0.3531 - accuracy: 0.9381\n",
            "Epoch 261/500\n",
            "17/17 - 0s - loss: 0.3585 - accuracy: 0.9362\n",
            "Epoch 262/500\n",
            "17/17 - 0s - loss: 0.3590 - accuracy: 0.9381\n",
            "Epoch 263/500\n",
            "17/17 - 0s - loss: 0.3527 - accuracy: 0.9362\n",
            "Epoch 264/500\n",
            "17/17 - 0s - loss: 0.3421 - accuracy: 0.9400\n",
            "Epoch 265/500\n",
            "17/17 - 0s - loss: 0.3580 - accuracy: 0.9284\n",
            "Epoch 266/500\n",
            "17/17 - 0s - loss: 0.3448 - accuracy: 0.9362\n",
            "Epoch 267/500\n",
            "17/17 - 0s - loss: 0.3469 - accuracy: 0.9420\n",
            "Epoch 268/500\n",
            "17/17 - 0s - loss: 0.3307 - accuracy: 0.9420\n",
            "Epoch 269/500\n",
            "17/17 - 0s - loss: 0.3266 - accuracy: 0.9439\n",
            "Epoch 270/500\n",
            "17/17 - 0s - loss: 0.3206 - accuracy: 0.9420\n",
            "Epoch 271/500\n",
            "17/17 - 0s - loss: 0.3207 - accuracy: 0.9439\n",
            "Epoch 272/500\n",
            "17/17 - 0s - loss: 0.3074 - accuracy: 0.9478\n",
            "Epoch 273/500\n",
            "17/17 - 0s - loss: 0.3005 - accuracy: 0.9497\n",
            "Epoch 274/500\n",
            "17/17 - 0s - loss: 0.2972 - accuracy: 0.9478\n",
            "Epoch 275/500\n",
            "17/17 - 0s - loss: 0.2935 - accuracy: 0.9478\n",
            "Epoch 276/500\n",
            "17/17 - 0s - loss: 0.2917 - accuracy: 0.9497\n",
            "Epoch 277/500\n",
            "17/17 - 0s - loss: 0.2881 - accuracy: 0.9478\n",
            "Epoch 278/500\n",
            "17/17 - 0s - loss: 0.2856 - accuracy: 0.9497\n",
            "Epoch 279/500\n",
            "17/17 - 0s - loss: 0.2834 - accuracy: 0.9516\n",
            "Epoch 280/500\n",
            "17/17 - 0s - loss: 0.2812 - accuracy: 0.9516\n",
            "Epoch 281/500\n",
            "17/17 - 0s - loss: 0.2800 - accuracy: 0.9516\n",
            "Epoch 282/500\n",
            "17/17 - 0s - loss: 0.2763 - accuracy: 0.9536\n",
            "Epoch 283/500\n",
            "17/17 - 0s - loss: 0.2738 - accuracy: 0.9536\n",
            "Epoch 284/500\n",
            "17/17 - 0s - loss: 0.2716 - accuracy: 0.9536\n",
            "Epoch 285/500\n",
            "17/17 - 0s - loss: 0.2698 - accuracy: 0.9536\n",
            "Epoch 286/500\n",
            "17/17 - 0s - loss: 0.2682 - accuracy: 0.9536\n",
            "Epoch 287/500\n",
            "17/17 - 0s - loss: 0.2658 - accuracy: 0.9536\n",
            "Epoch 288/500\n",
            "17/17 - 0s - loss: 0.2641 - accuracy: 0.9536\n",
            "Epoch 289/500\n",
            "17/17 - 0s - loss: 0.2630 - accuracy: 0.9516\n",
            "Epoch 290/500\n",
            "17/17 - 0s - loss: 0.2610 - accuracy: 0.9516\n",
            "Epoch 291/500\n",
            "17/17 - 0s - loss: 0.2594 - accuracy: 0.9497\n",
            "Epoch 292/500\n",
            "17/17 - 0s - loss: 0.2593 - accuracy: 0.9497\n",
            "Epoch 293/500\n",
            "17/17 - 0s - loss: 0.2583 - accuracy: 0.9516\n",
            "Epoch 294/500\n",
            "17/17 - 0s - loss: 0.2564 - accuracy: 0.9478\n",
            "Epoch 295/500\n",
            "17/17 - 0s - loss: 0.2536 - accuracy: 0.9497\n",
            "Epoch 296/500\n",
            "17/17 - 0s - loss: 0.2580 - accuracy: 0.9516\n",
            "Epoch 297/500\n",
            "17/17 - 0s - loss: 0.2598 - accuracy: 0.9478\n",
            "Epoch 298/500\n",
            "17/17 - 0s - loss: 0.2545 - accuracy: 0.9516\n",
            "Epoch 299/500\n",
            "17/17 - 0s - loss: 0.2501 - accuracy: 0.9516\n",
            "Epoch 300/500\n",
            "17/17 - 0s - loss: 0.2465 - accuracy: 0.9516\n",
            "Epoch 301/500\n",
            "17/17 - 0s - loss: 0.2424 - accuracy: 0.9536\n",
            "Epoch 302/500\n",
            "17/17 - 0s - loss: 0.2405 - accuracy: 0.9516\n",
            "Epoch 303/500\n",
            "17/17 - 0s - loss: 0.2400 - accuracy: 0.9555\n",
            "Epoch 304/500\n",
            "17/17 - 0s - loss: 0.2415 - accuracy: 0.9536\n",
            "Epoch 305/500\n",
            "17/17 - 0s - loss: 0.2530 - accuracy: 0.9478\n",
            "Epoch 306/500\n",
            "17/17 - 0s - loss: 0.2494 - accuracy: 0.9458\n",
            "Epoch 307/500\n",
            "17/17 - 0s - loss: 0.2438 - accuracy: 0.9478\n",
            "Epoch 308/500\n",
            "17/17 - 0s - loss: 0.2363 - accuracy: 0.9516\n",
            "Epoch 309/500\n",
            "17/17 - 0s - loss: 0.2324 - accuracy: 0.9497\n",
            "Epoch 310/500\n",
            "17/17 - 0s - loss: 0.2294 - accuracy: 0.9497\n",
            "Epoch 311/500\n",
            "17/17 - 0s - loss: 0.2272 - accuracy: 0.9497\n",
            "Epoch 312/500\n",
            "17/17 - 0s - loss: 0.2242 - accuracy: 0.9555\n",
            "Epoch 313/500\n",
            "17/17 - 0s - loss: 0.2231 - accuracy: 0.9516\n",
            "Epoch 314/500\n",
            "17/17 - 0s - loss: 0.2215 - accuracy: 0.9555\n",
            "Epoch 315/500\n",
            "17/17 - 0s - loss: 0.2201 - accuracy: 0.9555\n",
            "Epoch 316/500\n",
            "17/17 - 0s - loss: 0.2198 - accuracy: 0.9536\n",
            "Epoch 317/500\n",
            "17/17 - 0s - loss: 0.2187 - accuracy: 0.9555\n",
            "Epoch 318/500\n",
            "17/17 - 0s - loss: 0.2170 - accuracy: 0.9594\n",
            "Epoch 319/500\n",
            "17/17 - 0s - loss: 0.2155 - accuracy: 0.9555\n",
            "Epoch 320/500\n",
            "17/17 - 0s - loss: 0.2134 - accuracy: 0.9555\n",
            "Epoch 321/500\n",
            "17/17 - 0s - loss: 0.2131 - accuracy: 0.9536\n",
            "Epoch 322/500\n",
            "17/17 - 0s - loss: 0.2114 - accuracy: 0.9516\n",
            "Epoch 323/500\n",
            "17/17 - 0s - loss: 0.2095 - accuracy: 0.9555\n",
            "Epoch 324/500\n",
            "17/17 - 0s - loss: 0.2083 - accuracy: 0.9594\n",
            "Epoch 325/500\n",
            "17/17 - 0s - loss: 0.2076 - accuracy: 0.9574\n",
            "Epoch 326/500\n",
            "17/17 - 0s - loss: 0.2061 - accuracy: 0.9555\n",
            "Epoch 327/500\n",
            "17/17 - 0s - loss: 0.2049 - accuracy: 0.9555\n",
            "Epoch 328/500\n",
            "17/17 - 0s - loss: 0.2034 - accuracy: 0.9555\n",
            "Epoch 329/500\n",
            "17/17 - 0s - loss: 0.2026 - accuracy: 0.9555\n",
            "Epoch 330/500\n",
            "17/17 - 0s - loss: 0.2006 - accuracy: 0.9594\n",
            "Epoch 331/500\n",
            "17/17 - 0s - loss: 0.1992 - accuracy: 0.9594\n",
            "Epoch 332/500\n",
            "17/17 - 0s - loss: 0.1984 - accuracy: 0.9574\n",
            "Epoch 333/500\n",
            "17/17 - 0s - loss: 0.1971 - accuracy: 0.9594\n",
            "Epoch 334/500\n",
            "17/17 - 0s - loss: 0.1953 - accuracy: 0.9574\n",
            "Epoch 335/500\n",
            "17/17 - 0s - loss: 0.1943 - accuracy: 0.9594\n",
            "Epoch 336/500\n",
            "17/17 - 0s - loss: 0.1927 - accuracy: 0.9555\n",
            "Epoch 337/500\n",
            "17/17 - 0s - loss: 0.1917 - accuracy: 0.9594\n",
            "Epoch 338/500\n",
            "17/17 - 0s - loss: 0.1901 - accuracy: 0.9594\n",
            "Epoch 339/500\n",
            "17/17 - 0s - loss: 0.1903 - accuracy: 0.9574\n",
            "Epoch 340/500\n",
            "17/17 - 0s - loss: 0.1889 - accuracy: 0.9574\n",
            "Epoch 341/500\n",
            "17/17 - 0s - loss: 0.1881 - accuracy: 0.9574\n",
            "Epoch 342/500\n",
            "17/17 - 0s - loss: 0.1864 - accuracy: 0.9594\n",
            "Epoch 343/500\n",
            "17/17 - 0s - loss: 0.1859 - accuracy: 0.9594\n",
            "Epoch 344/500\n",
            "17/17 - 0s - loss: 0.1845 - accuracy: 0.9594\n",
            "Epoch 345/500\n",
            "17/17 - 0s - loss: 0.1836 - accuracy: 0.9574\n",
            "Epoch 346/500\n",
            "17/17 - 0s - loss: 0.1837 - accuracy: 0.9555\n",
            "Epoch 347/500\n",
            "17/17 - 0s - loss: 0.1810 - accuracy: 0.9574\n",
            "Epoch 348/500\n",
            "17/17 - 0s - loss: 0.1807 - accuracy: 0.9594\n",
            "Epoch 349/500\n",
            "17/17 - 0s - loss: 0.1802 - accuracy: 0.9555\n",
            "Epoch 350/500\n",
            "17/17 - 0s - loss: 0.1785 - accuracy: 0.9594\n",
            "Epoch 351/500\n",
            "17/17 - 0s - loss: 0.1777 - accuracy: 0.9536\n",
            "Epoch 352/500\n",
            "17/17 - 0s - loss: 0.1774 - accuracy: 0.9574\n",
            "Epoch 353/500\n",
            "17/17 - 0s - loss: 0.1767 - accuracy: 0.9574\n",
            "Epoch 354/500\n",
            "17/17 - 0s - loss: 0.1756 - accuracy: 0.9536\n",
            "Epoch 355/500\n",
            "17/17 - 0s - loss: 0.1739 - accuracy: 0.9555\n",
            "Epoch 356/500\n",
            "17/17 - 0s - loss: 0.1729 - accuracy: 0.9594\n",
            "Epoch 357/500\n",
            "17/17 - 0s - loss: 0.1724 - accuracy: 0.9555\n",
            "Epoch 358/500\n",
            "17/17 - 0s - loss: 0.1713 - accuracy: 0.9574\n",
            "Epoch 359/500\n",
            "17/17 - 0s - loss: 0.1708 - accuracy: 0.9574\n",
            "Epoch 360/500\n",
            "17/17 - 0s - loss: 0.1704 - accuracy: 0.9594\n",
            "Epoch 361/500\n",
            "17/17 - 0s - loss: 0.1691 - accuracy: 0.9594\n",
            "Epoch 362/500\n",
            "17/17 - 0s - loss: 0.1687 - accuracy: 0.9594\n",
            "Epoch 363/500\n",
            "17/17 - 0s - loss: 0.1671 - accuracy: 0.9574\n",
            "Epoch 364/500\n",
            "17/17 - 0s - loss: 0.1659 - accuracy: 0.9594\n",
            "Epoch 365/500\n",
            "17/17 - 0s - loss: 0.1654 - accuracy: 0.9574\n",
            "Epoch 366/500\n",
            "17/17 - 0s - loss: 0.1645 - accuracy: 0.9555\n",
            "Epoch 367/500\n",
            "17/17 - 0s - loss: 0.1636 - accuracy: 0.9594\n",
            "Epoch 368/500\n",
            "17/17 - 0s - loss: 0.1635 - accuracy: 0.9594\n",
            "Epoch 369/500\n",
            "17/17 - 0s - loss: 0.1624 - accuracy: 0.9594\n",
            "Epoch 370/500\n",
            "17/17 - 0s - loss: 0.1617 - accuracy: 0.9574\n",
            "Epoch 371/500\n",
            "17/17 - 0s - loss: 0.1607 - accuracy: 0.9594\n",
            "Epoch 372/500\n",
            "17/17 - 0s - loss: 0.1598 - accuracy: 0.9555\n",
            "Epoch 373/500\n",
            "17/17 - 0s - loss: 0.1592 - accuracy: 0.9594\n",
            "Epoch 374/500\n",
            "17/17 - 0s - loss: 0.1590 - accuracy: 0.9574\n",
            "Epoch 375/500\n",
            "17/17 - 0s - loss: 0.1593 - accuracy: 0.9574\n",
            "Epoch 376/500\n",
            "17/17 - 0s - loss: 0.1574 - accuracy: 0.9574\n",
            "Epoch 377/500\n",
            "17/17 - 0s - loss: 0.1568 - accuracy: 0.9574\n",
            "Epoch 378/500\n",
            "17/17 - 0s - loss: 0.1557 - accuracy: 0.9574\n",
            "Epoch 379/500\n",
            "17/17 - 0s - loss: 0.1549 - accuracy: 0.9594\n",
            "Epoch 380/500\n",
            "17/17 - 0s - loss: 0.1544 - accuracy: 0.9555\n",
            "Epoch 381/500\n",
            "17/17 - 0s - loss: 0.1534 - accuracy: 0.9594\n",
            "Epoch 382/500\n",
            "17/17 - 0s - loss: 0.1524 - accuracy: 0.9574\n",
            "Epoch 383/500\n",
            "17/17 - 0s - loss: 0.1522 - accuracy: 0.9574\n",
            "Epoch 384/500\n",
            "17/17 - 0s - loss: 0.1509 - accuracy: 0.9594\n",
            "Epoch 385/500\n",
            "17/17 - 0s - loss: 0.1504 - accuracy: 0.9574\n",
            "Epoch 386/500\n",
            "17/17 - 0s - loss: 0.1495 - accuracy: 0.9555\n",
            "Epoch 387/500\n",
            "17/17 - 0s - loss: 0.1488 - accuracy: 0.9555\n",
            "Epoch 388/500\n",
            "17/17 - 0s - loss: 0.1478 - accuracy: 0.9574\n",
            "Epoch 389/500\n",
            "17/17 - 0s - loss: 0.1473 - accuracy: 0.9516\n",
            "Epoch 390/500\n",
            "17/17 - 0s - loss: 0.1470 - accuracy: 0.9574\n",
            "Epoch 391/500\n",
            "17/17 - 0s - loss: 0.1465 - accuracy: 0.9594\n",
            "Epoch 392/500\n",
            "17/17 - 0s - loss: 0.1456 - accuracy: 0.9594\n",
            "Epoch 393/500\n",
            "17/17 - 0s - loss: 0.1461 - accuracy: 0.9574\n",
            "Epoch 394/500\n",
            "17/17 - 0s - loss: 0.1451 - accuracy: 0.9555\n",
            "Epoch 395/500\n",
            "17/17 - 0s - loss: 0.1445 - accuracy: 0.9555\n",
            "Epoch 396/500\n",
            "17/17 - 0s - loss: 0.1440 - accuracy: 0.9536\n",
            "Epoch 397/500\n",
            "17/17 - 0s - loss: 0.1434 - accuracy: 0.9555\n",
            "Epoch 398/500\n",
            "17/17 - 0s - loss: 0.1422 - accuracy: 0.9574\n",
            "Epoch 399/500\n",
            "17/17 - 0s - loss: 0.1416 - accuracy: 0.9594\n",
            "Epoch 400/500\n",
            "17/17 - 0s - loss: 0.1416 - accuracy: 0.9555\n",
            "Epoch 401/500\n",
            "17/17 - 0s - loss: 0.1409 - accuracy: 0.9574\n",
            "Epoch 402/500\n",
            "17/17 - 0s - loss: 0.1406 - accuracy: 0.9574\n",
            "Epoch 403/500\n",
            "17/17 - 0s - loss: 0.1392 - accuracy: 0.9516\n",
            "Epoch 404/500\n",
            "17/17 - 0s - loss: 0.1383 - accuracy: 0.9555\n",
            "Epoch 405/500\n",
            "17/17 - 0s - loss: 0.1374 - accuracy: 0.9536\n",
            "Epoch 406/500\n",
            "17/17 - 0s - loss: 0.1374 - accuracy: 0.9555\n",
            "Epoch 407/500\n",
            "17/17 - 0s - loss: 0.1371 - accuracy: 0.9555\n",
            "Epoch 408/500\n",
            "17/17 - 0s - loss: 0.1368 - accuracy: 0.9594\n",
            "Epoch 409/500\n",
            "17/17 - 0s - loss: 0.1493 - accuracy: 0.9536\n",
            "Epoch 410/500\n",
            "17/17 - 0s - loss: 0.2210 - accuracy: 0.9420\n",
            "Epoch 411/500\n",
            "17/17 - 0s - loss: 0.2166 - accuracy: 0.9420\n",
            "Epoch 412/500\n",
            "17/17 - 0s - loss: 0.2072 - accuracy: 0.9420\n",
            "Epoch 413/500\n",
            "17/17 - 0s - loss: 0.1772 - accuracy: 0.9516\n",
            "Epoch 414/500\n",
            "17/17 - 0s - loss: 0.2542 - accuracy: 0.9284\n",
            "Epoch 415/500\n",
            "17/17 - 0s - loss: 0.2917 - accuracy: 0.9091\n",
            "Epoch 416/500\n",
            "17/17 - 0s - loss: 0.2281 - accuracy: 0.9342\n",
            "Epoch 417/500\n",
            "17/17 - 0s - loss: 0.2118 - accuracy: 0.9439\n",
            "Epoch 418/500\n",
            "17/17 - 0s - loss: 0.1965 - accuracy: 0.9458\n",
            "Epoch 419/500\n",
            "17/17 - 0s - loss: 0.1952 - accuracy: 0.9420\n",
            "Epoch 420/500\n",
            "17/17 - 0s - loss: 0.2047 - accuracy: 0.9381\n",
            "Epoch 421/500\n",
            "17/17 - 0s - loss: 0.1801 - accuracy: 0.9478\n",
            "Epoch 422/500\n",
            "17/17 - 0s - loss: 0.1738 - accuracy: 0.9497\n",
            "Epoch 423/500\n",
            "17/17 - 0s - loss: 0.1651 - accuracy: 0.9458\n",
            "Epoch 424/500\n",
            "17/17 - 0s - loss: 0.1612 - accuracy: 0.9516\n",
            "Epoch 425/500\n",
            "17/17 - 0s - loss: 0.1593 - accuracy: 0.9516\n",
            "Epoch 426/500\n",
            "17/17 - 0s - loss: 0.1500 - accuracy: 0.9574\n",
            "Epoch 427/500\n",
            "17/17 - 0s - loss: 0.1474 - accuracy: 0.9536\n",
            "Epoch 428/500\n",
            "17/17 - 0s - loss: 0.1453 - accuracy: 0.9555\n",
            "Epoch 429/500\n",
            "17/17 - 0s - loss: 0.1428 - accuracy: 0.9574\n",
            "Epoch 430/500\n",
            "17/17 - 0s - loss: 0.1412 - accuracy: 0.9555\n",
            "Epoch 431/500\n",
            "17/17 - 0s - loss: 0.1402 - accuracy: 0.9594\n",
            "Epoch 432/500\n",
            "17/17 - 0s - loss: 0.1383 - accuracy: 0.9536\n",
            "Epoch 433/500\n",
            "17/17 - 0s - loss: 0.1369 - accuracy: 0.9574\n",
            "Epoch 434/500\n",
            "17/17 - 0s - loss: 0.1348 - accuracy: 0.9574\n",
            "Epoch 435/500\n",
            "17/17 - 0s - loss: 0.1356 - accuracy: 0.9555\n",
            "Epoch 436/500\n",
            "17/17 - 0s - loss: 0.1352 - accuracy: 0.9594\n",
            "Epoch 437/500\n",
            "17/17 - 0s - loss: 0.1381 - accuracy: 0.9555\n",
            "Epoch 438/500\n",
            "17/17 - 0s - loss: 0.1335 - accuracy: 0.9516\n",
            "Epoch 439/500\n",
            "17/17 - 0s - loss: 0.1330 - accuracy: 0.9555\n",
            "Epoch 440/500\n",
            "17/17 - 0s - loss: 0.1317 - accuracy: 0.9536\n",
            "Epoch 441/500\n",
            "17/17 - 0s - loss: 0.1307 - accuracy: 0.9555\n",
            "Epoch 442/500\n",
            "17/17 - 0s - loss: 0.1299 - accuracy: 0.9574\n",
            "Epoch 443/500\n",
            "17/17 - 0s - loss: 0.1292 - accuracy: 0.9574\n",
            "Epoch 444/500\n",
            "17/17 - 0s - loss: 0.1287 - accuracy: 0.9574\n",
            "Epoch 445/500\n",
            "17/17 - 0s - loss: 0.1283 - accuracy: 0.9555\n",
            "Epoch 446/500\n",
            "17/17 - 0s - loss: 0.1275 - accuracy: 0.9555\n",
            "Epoch 447/500\n",
            "17/17 - 0s - loss: 0.1268 - accuracy: 0.9555\n",
            "Epoch 448/500\n",
            "17/17 - 0s - loss: 0.1272 - accuracy: 0.9574\n",
            "Epoch 449/500\n",
            "17/17 - 0s - loss: 0.1259 - accuracy: 0.9555\n",
            "Epoch 450/500\n",
            "17/17 - 0s - loss: 0.1256 - accuracy: 0.9555\n",
            "Epoch 451/500\n",
            "17/17 - 0s - loss: 0.1253 - accuracy: 0.9574\n",
            "Epoch 452/500\n",
            "17/17 - 0s - loss: 0.1248 - accuracy: 0.9555\n",
            "Epoch 453/500\n",
            "17/17 - 0s - loss: 0.1239 - accuracy: 0.9555\n",
            "Epoch 454/500\n",
            "17/17 - 0s - loss: 0.1248 - accuracy: 0.9594\n",
            "Epoch 455/500\n",
            "17/17 - 0s - loss: 0.1238 - accuracy: 0.9536\n",
            "Epoch 456/500\n",
            "17/17 - 0s - loss: 0.1236 - accuracy: 0.9516\n",
            "Epoch 457/500\n",
            "17/17 - 0s - loss: 0.1226 - accuracy: 0.9555\n",
            "Epoch 458/500\n",
            "17/17 - 0s - loss: 0.1224 - accuracy: 0.9536\n",
            "Epoch 459/500\n",
            "17/17 - 0s - loss: 0.1221 - accuracy: 0.9536\n",
            "Epoch 460/500\n",
            "17/17 - 0s - loss: 0.1213 - accuracy: 0.9516\n",
            "Epoch 461/500\n",
            "17/17 - 0s - loss: 0.1219 - accuracy: 0.9555\n",
            "Epoch 462/500\n",
            "17/17 - 0s - loss: 0.1208 - accuracy: 0.9555\n",
            "Epoch 463/500\n",
            "17/17 - 0s - loss: 0.1215 - accuracy: 0.9536\n",
            "Epoch 464/500\n",
            "17/17 - 0s - loss: 0.1201 - accuracy: 0.9536\n",
            "Epoch 465/500\n",
            "17/17 - 0s - loss: 0.1197 - accuracy: 0.9574\n",
            "Epoch 466/500\n",
            "17/17 - 0s - loss: 0.1192 - accuracy: 0.9497\n",
            "Epoch 467/500\n",
            "17/17 - 0s - loss: 0.1187 - accuracy: 0.9555\n",
            "Epoch 468/500\n",
            "17/17 - 0s - loss: 0.1185 - accuracy: 0.9516\n",
            "Epoch 469/500\n",
            "17/17 - 0s - loss: 0.1182 - accuracy: 0.9574\n",
            "Epoch 470/500\n",
            "17/17 - 0s - loss: 0.1186 - accuracy: 0.9536\n",
            "Epoch 471/500\n",
            "17/17 - 0s - loss: 0.1179 - accuracy: 0.9516\n",
            "Epoch 472/500\n",
            "17/17 - 0s - loss: 0.1173 - accuracy: 0.9574\n",
            "Epoch 473/500\n",
            "17/17 - 0s - loss: 0.1166 - accuracy: 0.9574\n",
            "Epoch 474/500\n",
            "17/17 - 0s - loss: 0.1165 - accuracy: 0.9594\n",
            "Epoch 475/500\n",
            "17/17 - 0s - loss: 0.1152 - accuracy: 0.9574\n",
            "Epoch 476/500\n",
            "17/17 - 0s - loss: 0.1156 - accuracy: 0.9555\n",
            "Epoch 477/500\n",
            "17/17 - 0s - loss: 0.1153 - accuracy: 0.9536\n",
            "Epoch 478/500\n",
            "17/17 - 0s - loss: 0.1144 - accuracy: 0.9574\n",
            "Epoch 479/500\n",
            "17/17 - 0s - loss: 0.1157 - accuracy: 0.9536\n",
            "Epoch 480/500\n",
            "17/17 - 0s - loss: 0.1146 - accuracy: 0.9574\n",
            "Epoch 481/500\n",
            "17/17 - 0s - loss: 0.1145 - accuracy: 0.9594\n",
            "Epoch 482/500\n",
            "17/17 - 0s - loss: 0.1144 - accuracy: 0.9555\n",
            "Epoch 483/500\n",
            "17/17 - 0s - loss: 0.1137 - accuracy: 0.9516\n",
            "Epoch 484/500\n",
            "17/17 - 0s - loss: 0.1137 - accuracy: 0.9574\n",
            "Epoch 485/500\n",
            "17/17 - 0s - loss: 0.1130 - accuracy: 0.9497\n",
            "Epoch 486/500\n",
            "17/17 - 0s - loss: 0.1133 - accuracy: 0.9555\n",
            "Epoch 487/500\n",
            "17/17 - 0s - loss: 0.1136 - accuracy: 0.9516\n",
            "Epoch 488/500\n",
            "17/17 - 0s - loss: 0.1125 - accuracy: 0.9574\n",
            "Epoch 489/500\n",
            "17/17 - 0s - loss: 0.1123 - accuracy: 0.9555\n",
            "Epoch 490/500\n",
            "17/17 - 0s - loss: 0.1119 - accuracy: 0.9536\n",
            "Epoch 491/500\n",
            "17/17 - 0s - loss: 0.1115 - accuracy: 0.9594\n",
            "Epoch 492/500\n",
            "17/17 - 0s - loss: 0.1114 - accuracy: 0.9555\n",
            "Epoch 493/500\n",
            "17/17 - 0s - loss: 0.1114 - accuracy: 0.9536\n",
            "Epoch 494/500\n",
            "17/17 - 0s - loss: 0.1111 - accuracy: 0.9574\n",
            "Epoch 495/500\n",
            "17/17 - 0s - loss: 0.1119 - accuracy: 0.9555\n",
            "Epoch 496/500\n",
            "17/17 - 0s - loss: 0.1106 - accuracy: 0.9536\n",
            "Epoch 497/500\n",
            "17/17 - 0s - loss: 0.1100 - accuracy: 0.9574\n",
            "Epoch 498/500\n",
            "17/17 - 0s - loss: 0.1094 - accuracy: 0.9555\n",
            "Epoch 499/500\n",
            "17/17 - 0s - loss: 0.1088 - accuracy: 0.9594\n",
            "Epoch 500/500\n",
            "17/17 - 0s - loss: 0.1088 - accuracy: 0.9574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "jwagmpTiZ6K2",
        "outputId": "d4f0671b-443e-4bf7-c3d3-dcf13c3a72f6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgzW-9r-boSl",
        "outputId": "4c9e269a-ab6a-4f01-c209-14c6579c73f0"
      },
      "source": [
        "seed_text = \"Laurence arrived with\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequences_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print('\\n\\n',seed_text)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Laurence arrived with girls and dancing too much i suppose suppose he too call jig cask boys relations relations relations relations relations entangled entangled hearty entangled entangled twas small small small drop drop drop drop too and listen ill make your your eyes glisten glisten glisten glisten glisten glisten glisten gray steps steps time for lanigans ball ball ball ball ball ball ball wall wall wall youd hall hall hall eyes nothing together merry steps steps a fainted relations groups her lanigans ball ball fainted relations relations relations all mad at by her relations relations steps a cask rose milliner pound rose too\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}