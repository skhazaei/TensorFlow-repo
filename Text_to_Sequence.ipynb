{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-to-Sequence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7ZtdHNZmzg0gTmrd7bnGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skhazaei/TensorFlow-repo/blob/master/Text_to_Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2sbJ4WEyzr-",
        "outputId": "7d224bb2-5f5a-4da5-8ba9-0cbea8dc8d1b"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "sentences = ['I love my cat',\n",
        "             'I love my dog']\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print (word_index)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'cat': 4, 'dog': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMMsTHBU4qn4"
      },
      "source": [
        "# Convert the sentences to a sequence of numeric values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUfLWiCF4pWm",
        "outputId": "b0f01b19-dc0d-4c0c-d39a-c34ea5ab0977"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print (sequences)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4], [1, 2, 3, 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j35xnCYK5bnT"
      },
      "source": [
        "# Let's try some test sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHRZ3Jd05JJ2",
        "outputId": "6b6d3514-307f-4258-95b8-e8424306280f"
      },
      "source": [
        "test_sentences = ['I really love my CAT',\n",
        "                  'my cat is lovely!']\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "print (test_seq)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4], [3, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCAdRcW26DCs"
      },
      "source": [
        "##As it can be seen, the unseen words of test set are ignored. To fix this, we can add some value for unseen words using a property in Tokenizer, so lets rewrite the first code cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHdia09z5j7k",
        "outputId": "701c019e-bcd4-43a2-ab36-2de3c13c9dfb"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "sentences = ['I love my cat',\n",
        "             'I love my dog']\n",
        "             \n",
        "# ovv: out of vocabulary\n",
        "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print('word index: {}'.format(word_index))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print ('sentences to sequences of numeric values: {}'.format(sequences))\n",
        "\n",
        "# Try with test sentences\n",
        "test_sentences = ['I really love my CAT',\n",
        "                  'my cat is lovely!']\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "print ('texts-to-sequences for test sentences: {}'.format(test_seq))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word index: {'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'cat': 5, 'dog': 6}\n",
            "sentences to sequences of numeric values: [[2, 3, 4, 5], [2, 3, 4, 6]]\n",
            "texts-to-sequences for test sentences: [[2, 1, 3, 4, 5], [4, 5, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}