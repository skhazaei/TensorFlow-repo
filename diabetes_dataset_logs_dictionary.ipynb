{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diabetes_dataset_logs_dictionary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlD7Bce/bq/mmC32lhXyjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skhazaei/TensorFlow-repo/blob/master/diabetes_dataset_logs_dictionary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnsyjToWnib4"
      },
      "source": [
        "# Check [here](https://keras.io/callbacks/) for custom callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TynVqUqzcuIR",
        "outputId": "ed71afd7-c2a2-43bb-c1f0-7614f98e9440"
      },
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "diabetes_dataset = load_diabetes()\n",
        "diabetes_dataset.keys()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75wAoHbkdb5I"
      },
      "source": [
        "data = diabetes_dataset['data']\n",
        "target = diabetes_dataset['target']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Ro9A4edj7u"
      },
      "source": [
        "target = (target - target.mean(axis=0)) / target.std()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHW-j7modsI9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=0.1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dFSucL-hWwd",
        "outputId": "e68ef74a-0b40-4d6e-97d4-b9e19975931f"
      },
      "source": [
        "print(f'train data shape: {train_data.shape}')\n",
        "print(f'test data shape: {test_data.shape}')\n",
        "print(f'train target shape: {train_target.shape}')\n",
        "print(f'test target shape: {test_target.shape}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data shape: (397, 10)\n",
            "test data shape: (45, 10)\n",
            "train target shape: (397,)\n",
            "test target shape: (45,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA5xt2d5d9aZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
        "    Dense(64,activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)        \n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUkb_17OfGzl",
        "outputId": "191b70ef-0b77-4961-fba2-0e9088c3247a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               1408      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 18,305\n",
            "Trainable params: 18,177\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKlr8gPOfQDr"
      },
      "source": [
        "model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpwBva5hrIB"
      },
      "source": [
        "# Let's create a custom callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAOK-SzgfWTE"
      },
      "source": [
        "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # Print the loss every two steps of training\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if batch %2 ==0:\n",
        "          print('After batch {}, the loss is {}'.format(batch, logs['loss']))\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "      print('Start of epoch {}'.format(epoch+1))\n",
        "    \n",
        "    # Print the loss after each batch in the test set\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "      print('After batch {}, the loss is {}'.format(batch, logs['loss']))\n",
        "\n",
        "    # Print the loss and mean absolute error after each epoch\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      print('End of epoch {}: Average loss is {}, and mean absolute error is {}'.format(epoch+1, logs['loss'], logs['mae']))\n",
        "    \n",
        "    # Notify the user when prediction has finished on each batch\n",
        "    def on_predict_batch_end(self,batch, logs=None):\n",
        "        print(\"Finished prediction on batch {}!\".format(batch))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK5R5rwffj-7",
        "outputId": "ae2255cd-876e-4b7b-8353-9a49b4cab29f"
      },
      "source": [
        "history = model.fit(train_data, train_target, epochs=2,\n",
        "                    batch_size=64, callbacks=[LossAndMetricCallback()], verbose=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "After batch 0, the loss is 1.0404589176177979\n",
            "After batch 2, the loss is 0.9033668637275696\n",
            "After batch 4, the loss is 0.870500922203064\n",
            "After batch 6, the loss is 0.8140500783920288\n",
            "End of epoch 1: Average loss is 0.8140500783920288, and mean absolute error is 0.7629238367080688\n",
            "Start of epoch 2\n",
            "After batch 0, the loss is 0.6351763606071472\n",
            "After batch 2, the loss is 0.554715096950531\n",
            "After batch 4, the loss is 0.5279847383499146\n",
            "After batch 6, the loss is 0.5434674024581909\n",
            "End of epoch 2: Average loss is 0.5434674024581909, and mean absolute error is 0.6010139584541321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-OtBuUefmXQ",
        "outputId": "70c99827-4cfd-4228-ad57-e3cc5761cc55"
      },
      "source": [
        "model_eval = model.evaluate(test_data, test_target, batch_size=10, \n",
        "                            callbacks=[LossAndMetricCallback()], verbose=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After batch 0, the loss is 0.9949547648429871\n",
            "After batch 1, the loss is 0.7982174158096313\n",
            "After batch 2, the loss is 0.7012218236923218\n",
            "After batch 3, the loss is 0.8305608034133911\n",
            "After batch 4, the loss is 0.8850511312484741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY4xiXqCftv8",
        "outputId": "cd3bd624-b8e8-4f6f-c791-6f826fc7fc2f"
      },
      "source": [
        "model_pred = model.predict(test_data, batch_size=10,\n",
        "                           callbacks=[LossAndMetricCallback()], verbose=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished prediction on batch 0!\n",
            "Finished prediction on batch 1!\n",
            "Finished prediction on batch 2!\n",
            "Finished prediction on batch 3!\n",
            "Finished prediction on batch 4!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkJqZ2Q7j18A"
      },
      "source": [
        "# A more sophisticated custom callback: Learning rate scheduler \n",
        "\n",
        "To define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMcy8iLXfw3d"
      },
      "source": [
        "# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n",
        "lr_schedule = [(4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)]\n",
        "\n",
        "\n",
        "def get_new_epoch_lr(epoch, lr):\n",
        "    # Checks to see if the input epoch is listed in the learning rate schedule \n",
        "    # and if so, returns index in lr_schedule\n",
        "    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0]==int(epoch)]\n",
        "    if len(epoch_in_sched)>0:\n",
        "        new_lr = lr_schedule[epoch_in_sched[0]][1]      \n",
        "        # If it is, return the learning rate corresponding to the epoch\n",
        "        return new_lr\n",
        "    else:\n",
        "        # Otherwise, return the existing learning rate\n",
        "        return lr"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2tUDcQtkTWb"
      },
      "source": [
        "class LRScheduler(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, new_lr):\n",
        "        super(LRScheduler, self).__init__()\n",
        "        # Add the new learning rate function to our callback\n",
        "        self.new_lr = new_lr\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "              raise ValueError('Error: Optimizer does not have a learning rate.')\n",
        "                \n",
        "        # Get the current learning rate\n",
        "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "        \n",
        "        # Call the auxillary function to get the scheduled learning rate for the current epoch\n",
        "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
        "\n",
        "        # Set the learning rate to the scheduled learning rate\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
        "        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTicnDmRkVgU"
      },
      "source": [
        "new_model = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
        "    Dense(64,activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)        \n",
        "])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI4662OVkXYL"
      },
      "source": [
        "new_model.compile(loss='mse',\n",
        "                optimizer=\"adam\",\n",
        "                metrics=['mae', 'mse'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjkCeOigkZEg",
        "outputId": "939c64f0-2c4f-46e3-d2ab-4459d8c660c9"
      },
      "source": [
        "new_history = new_model.fit(train_data, train_target, epochs=20,\n",
        "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate for epoch 0 is   0.001\n",
            "Learning rate for epoch 1 is   0.001\n",
            "Learning rate for epoch 2 is   0.001\n",
            "Learning rate for epoch 3 is   0.001\n",
            "Learning rate for epoch 4 is   0.030\n",
            "Learning rate for epoch 5 is   0.030\n",
            "Learning rate for epoch 6 is   0.030\n",
            "Learning rate for epoch 7 is   0.020\n",
            "Learning rate for epoch 8 is   0.020\n",
            "Learning rate for epoch 9 is   0.020\n",
            "Learning rate for epoch 10 is   0.020\n",
            "Learning rate for epoch 11 is   0.005\n",
            "Learning rate for epoch 12 is   0.005\n",
            "Learning rate for epoch 13 is   0.005\n",
            "Learning rate for epoch 14 is   0.005\n",
            "Learning rate for epoch 15 is   0.007\n",
            "Learning rate for epoch 16 is   0.007\n",
            "Learning rate for epoch 17 is   0.007\n",
            "Learning rate for epoch 18 is   0.007\n",
            "Learning rate for epoch 19 is   0.007\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}